{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 001 - Part I\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 15/08/2023 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_08/Exercise0001Part001.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    " - Fill the full names of the team memebers in the `Team Members` section.\n",
    " - Answer all questions within the Jupyter Notebook.\n",
    " - Open questions are in part I of the exercise.\n",
    " - Coding based questions are in the subsequent notebooks.\n",
    " - Use MarkDown + MathJaX + Code to answer.\n",
    " - Submission in groups (Single submission per group).\n",
    " - You may and _should_ use the forums for question.\n",
    " - Good Luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "\n",
    " - `<Ilan>_<schaedel>_<011708005>`.\n",
    " - `<Barak>_<Ben Menachem>_<025372830>`.\n",
    " - `<Rotem>_<Wiasman>_<203958103>`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Linear Algebra\n",
    "\n",
    "A matrix $ P $ is called an orthogonal projection operator if, and only if it is idempotent and symmetric.\n",
    "\n",
    "**Remark**: Idempotent matrix means $ \\forall n \\in \\mathcal{N} \\; {P}^{n} = P $.\n",
    "\n",
    "### 0.1. Question\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$ where $ m \\geq n $ and $ \\operatorname{Rank} \\left( A \\right) = n $.  \n",
    "Given the linear least squares problem:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| A \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2} $$\n",
    "\n",
    "With the solution in the form $\\hat{\\boldsymbol{x}} = R \\boldsymbol{y}$, show that $P = A R$ is an orthogonal projection operator.\n",
    "\n",
    "**Hints**\n",
    "\n",
    "1. Derive the solution to the Least Squares above in the form of $ \\hat{\\boldsymbol{x}} = R \\boldsymbol{y} $.\n",
    "2. Show the $ P $ matrix is symmetric.\n",
    "3. Show the $ P $ matrix is idempotent.\n",
    "4. Conclude the matrix is an orthogonal projection operator.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The matrix $P$ is the Orthogonal Projection onto the range (Columns space) of $ A $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Solution\n",
    "\n",
    "General solution to the problem:\n",
    "1. derive the argmin function: $\\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| A \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2}$\n",
    "2. find the solution for $\\nabla f\\left(\\boldsymbol{x}\\right)=0$\n",
    "\n",
    "\n",
    "To calculate $\\nabla f\\left(\\boldsymbol{x}\\right)=0$:\n",
    "\n",
    "\n",
    "a. $ A^T (A \\boldsymbol{x} - \\boldsymbol{y}) = 0 $\n",
    "\n",
    "b. $ A^T A \\boldsymbol{x} - A^T \\boldsymbol{y} = 0 $\n",
    "\n",
    "c. b. $ A^T A \\boldsymbol{x} = A^T \\boldsymbol{y} $\n",
    "\n",
    "d. $ \\hat{\\boldsymbol{x}} = (A^T A)^{-1} A^T \\boldsymbol{y} $\n",
    "\n",
    "Now we can denote the least squares problem: \n",
    "$\\hat{\\boldsymbol{x}} = R \\boldsymbol{y}$ where $R = (A^T A)^{-1} A^T$.\n",
    "\n",
    "let $P = A R$. So  $P = A (A^T A)^{-1} A^T$. \n",
    "\n",
    "To show $P$ is an orthogonal projection to be prooved by induction :\n",
    "1. For example show: that $P^2 = P$ \n",
    "2. Show for all cases for $\\forall n$  \n",
    "\n",
    "\n",
    "$P^2 = P$:\n",
    "\n",
    "   $P^2 = (A (A^T A)^{-1} A^T)(A (A^T A)^{-1} A^T) = A (A^T A)^{-1} (A^T A) (A^T A)^{-1} A^T = A (A^T A)^{-1} A^T = P $\n",
    "\n",
    "$P^{n+1} = P^n$:\n",
    "\n",
    "   $P^{n+1} = (A (A^T A)^{-1} A^T)^n(A (A^T A)^{-1} A^T) = A (A^T A)^{-1} (A^T A) (A^T A)^{-1} A^T = A (A^T A)^{-1} A^T = P^n $\n",
    "\n",
    "\n",
    "\n",
    "$P$ is symmetric:\n",
    "\n",
    "$P^{T}=\\left(A(A^{T}A)^{-1}A^{T}\\right)^{T}=A\\left((A^{T}A)^{-1}\\right)^{T}A^{T}=A\\left((A^{T}A)^{T}\\right)^{-1}A^{T}=A(A^{T}A)^{-1}A^{T}=P$\n",
    "\n",
    "$P$ is an orthogonal projection operator.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An orthogonal projection $ \\boldsymbol{P} $ onto a sub space $ V $ obeys:\n",
    "\n",
    "1. $ \\forall \\boldsymbol{w} : \\boldsymbol{P} \\boldsymbol{w} \\in V $.\n",
    "2. $ \\boldsymbol{w} - \\boldsymbol{P} \\boldsymbol{w} \\in {V}^{\\bot} $.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> If $ \\boldsymbol{v} \\in {V}, \\boldsymbol{u} \\in {V}^{\\bot} $ then $ \\left \\langle \\boldsymbol{u}, \\boldsymbol{v} \\right \\rangle = 0 $.\n",
    "\n",
    "The orthogonal projection onto the set $ \\left\\{ \\boldsymbol{x} \\mid \\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b} \\right\\} $ is given by:\n",
    "\n",
    "$$ \\operatorname{Proj}_{ \\left\\{ \\boldsymbol{x} \\mid \\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b} \\right\\} } \\left( \\boldsymbol{z} \\right) = \\boldsymbol{z} - \\boldsymbol{A}^{T} {\\left( \\boldsymbol{A} \\boldsymbol{A}^{T} \\right)}^{-1} \\left( \\boldsymbol{A} \\boldsymbol{z} - \\boldsymbol{b} \\right) $$\n",
    "\n",
    "### 0.2. Question  \n",
    "\n",
    "Prove or disprove: The above is orthogonal projection onto a sub space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Solution\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "&\\text{To prove the given projection is an orthogonal projection onto a subspace, we must verify the two properties provided.} \\\\\n",
    "\n",
    "&1. \\textbf{Property 1}: \\\\\n",
    "&\\forall \\boldsymbol{w} : \\boldsymbol{P} \\boldsymbol{w} \\in V \\\\\n",
    "\n",
    "&\\text{For the given projection:} \\\\\n",
    "&\\operatorname{Proj}_{ \\left\\{ \\boldsymbol{x} \\mid \\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b} \\right\\} } \\left( \\boldsymbol{z} \\right) = \\boldsymbol{z} - \\boldsymbol{A}^{T} {\\left( \\boldsymbol{A} \\boldsymbol{A}^{T} \\right)}^{-1} \\left( \\boldsymbol{A} \\boldsymbol{z} - \\boldsymbol{b} \\right) \\\\\n",
    "\n",
    "&\\text{Let's denote this as:} \\\\\n",
    "&\\boldsymbol{p}(\\boldsymbol{z}) \\\\\n",
    "\n",
    "&\\text{To prove that } \\boldsymbol{p}(\\boldsymbol{z}) \\in V, \\text{ it should satisfy:} \\\\\n",
    "&\\boldsymbol{A} \\boldsymbol{p}(\\boldsymbol{z}) = \\boldsymbol{b} \\\\\n",
    "\n",
    "&\\Rightarrow \\boldsymbol{A} \\left( \\boldsymbol{z} - \\boldsymbol{A}^{T} {\\left( \\boldsymbol{A} \\boldsymbol{A}^{T} \\right)}^{-1} \\left( \\boldsymbol{A} \\boldsymbol{z} - \\boldsymbol{b} \\right) \\right) = \\boldsymbol{b} \\\\\n",
    "\n",
    "&2. \\textbf{Property 2}: \\\\\n",
    "&\\boldsymbol{w} - \\boldsymbol{P} \\boldsymbol{w} \\in {V}^{\\bot} \\\\\n",
    "\n",
    "&\\text{To show that the residual vector } \\boldsymbol{r} = \\boldsymbol{z} - \\boldsymbol{p}(\\boldsymbol{z}) \\text{ is orthogonal to } V, \\text{ we can verify:} \\\\\n",
    "&\\left \\langle \\boldsymbol{r}, \\boldsymbol{v} \\right \\rangle = 0 \\\\\n",
    "\n",
    "&\\text{for any } \\boldsymbol{v} \\in V. \\text{ Where } \\left \\langle \\cdot, \\cdot \\right \\rangle \\text{ denotes the dot product.} \\\\\n",
    "&\\Rightarrow \\left \\langle \\boldsymbol{z} - \\boldsymbol{p}(\\boldsymbol{z}), \\boldsymbol{v} \\right \\rangle = \\left \\langle \\boldsymbol{A}^{T} {\\left( \\boldsymbol{A} \\boldsymbol{A}^{T} \\right)}^{-1} \\left( \\boldsymbol{A} \\boldsymbol{z} - \\boldsymbol{b} \\right), \\boldsymbol{v} \\right \\rangle = 0 \\\\\n",
    "\n",
    "&\\text{Hence, both properties of the orthogonal projection are satisfied by the given projection.}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convexity\n",
    "\n",
    "**Convex Set**  \n",
    "\n",
    "Let:\n",
    "\n",
    "$$ \\mathbb{R}_{\\geq 0}^{d} = \\left\\{ \\boldsymbol{x} \\in\\mathbb{R}^{d} \\, \\bigg| \\, \\min_{i} {x}_{i} \\geq 0 \\right\\} $$\n",
    "\n",
    "Where $\\boldsymbol{x} = \\begin{bmatrix} {x}_{1} \\\\ {x}_{2} \\\\ \\vdots \\\\ {x}_{d} \\end{bmatrix}$\n",
    "\n",
    "### 1.1. Question\n",
    "\n",
    "Prove or disprove that $\\mathbb{R}_{\\geq 0}^{d}$ is convex."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Solution\n",
    "\n",
    "Let $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R^4}\\geq0$\n",
    "\n",
    "Define $\\boldsymbol{z}=:\\alpha\\boldsymbol{x}+(1-\\alpha)\\boldsymbol{y}$\n",
    "\n",
    "s.t. $\\alpha\\in[0,1]$\n",
    "\n",
    "$\\min\\limits_{i} {x}_{i}, \\min\\limits_{i} y_{i}\\geq0\\Rightarrow z_{i}\\geq0 , \\forall i$\n",
    "\n",
    "$\\Rightarrow\\min\\limits_{i} z_{i} \\geq 0 $\n",
    "\n",
    "\n",
    "$\\Rightarrow \\boldsymbol{z}\\in\\mathbb{R}_{\\geq0}^{d}$\n",
    "\n",
    "$\\mathbb{R}_{\\geq0}^{d}$ is convex.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convex Combination** \n",
    "\n",
    "Let $\\mathcal{C} \\subseteq \\mathbb{R}^{d} $ be a convex set and consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\} _{i=1}^{N}$.\n",
    "\n",
    "### 1.2. Question\n",
    "\n",
    "Prove that for any $N \\in \\mathbb{N}$: \n",
    "\n",
    "$$ \\sum_{i = 1}^{N} {\\alpha}_{i} \\boldsymbol{x}_{i} \\in \\mathcal{C} $$\n",
    "\n",
    "Where $\\alpha_{i}$ are such that: \n",
    "\n",
    " - $\\forall i, \\; \\alpha_{i} \\geq 0$.\n",
    " - $\\sum_{i = 1}^{N} \\alpha_{i} = 1$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The properties of ${\\alpha}_{i}$ above means it is sampled from the Unit Probability Simplex.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Solution\n",
    "\n",
    "Solution by induction on $N$:\n",
    "\n",
    "**Base case**: $N = 1:$\n",
    "\n",
    "Only one $\\boldsymbol{x}$: $\\alpha \\boldsymbol{x} \\in \\mathcal{C}$\n",
    "\n",
    "**Induction step:** \n",
    "\n",
    "Assume $N-1$ combinations are also $\\in \\mathcal{C}$, let's prove: $ \\sum_{i = 1}^{N} {\\alpha}_{i} \\boldsymbol{x}_{i} \\in \\mathcal{C} $\n",
    "\n",
    "By the inductive hypothesis:\n",
    " \n",
    "$\\boldsymbol{y} = \\sum_{j=1}^{N-1}\\frac{\\alpha_{j}}{\\sum_{i=1}^{N-1}\\alpha_{i}}\\boldsymbol{x_{j}} \\in \\mathcal{C}$ \n",
    "(assuming $\\sum_{i=1}^{N-1}\\alpha_{i}\\neq 0$; Otherwise, $\\alpha_{N}$ is the only nonzero coefficient - solved by Base case)\n",
    "\n",
    "We can rewrite the N convex combinations:\n",
    "\n",
    "$ \\sum_{i = 1}^{N} {\\alpha}_{i} \\boldsymbol{x}_{i} = \\sum_{i=1}^{N-1}\\alpha_{i}\\boldsymbol{y} + \\alpha_{N}\\boldsymbol{x}$, Or:\n",
    "\n",
    "$(1-\\alpha_{N})\\boldsymbol{y} + \\alpha_{N}\\boldsymbol{x}$ which by definition is $\\in \\mathcal{C} $\n",
    "\n",
    "Thus, by induction, convex combinations of all size $\\in \\mathcal{C}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal{C}\\subset\\mathbb{R}^{2}$ be a convex set.  \n",
    "Consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\}_{i=1}^{10}$ such that $\\boldsymbol{x}_{i} \\neq \\boldsymbol{x}_{j}$ for all $i \\neq j$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Question\n",
    "\n",
    "Prove or disprove the following assertion:\n",
    "\n",
    "Necessarily, any point $\\boldsymbol{y} \\in \\mathcal{C}$ can be represented as a convex combination of $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Solution\n",
    "\n",
    "False accertion:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\textbf{Counterexample:} \\\\\n",
    "\n",
    "&\\text{Let's choose the points } \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{4} \\text{ in } \\mathbb{R}^2 \\text{ as:} \\\\\n",
    "&\\boldsymbol{x}_{1} = (0, 0) \\\\\n",
    "&\\boldsymbol{x}_{2} = (1, 0) \\\\\n",
    "&\\boldsymbol{x}_{3} = (1, 1) \\\\\n",
    "&\\boldsymbol{x}_{4} = (0, 1) \\\\\n",
    "\n",
    "&\\text{The convex hull of these 4 points is a square with vertices at these 4 points.} \\\\\n",
    "\n",
    "&\\text{Now, for } \\mathcal{C}, \\text{ let's define it as a larger square that contains the convex hull of } \\\\\n",
    "&\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{4} \\text{ but also has points outside of it. Let's set:} \\\\\n",
    "&\\mathcal{C} = \\{ (x, y) \\mid -0.5 \\leq x \\leq 1.5 \\text{ and } -0.5 \\leq y \\leq 1.5 \\} \\\\\n",
    "\n",
    "&\\text{Clearly, a point like } \\boldsymbol{y} = (-0.5, -0.5) \\text{ is inside } \\mathcal{C} \\\\\n",
    "&\\text{but is outside the convex hull of } \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{4}. \\\\\n",
    "&\\text{Therefore, } \\boldsymbol{y} \\text{ cannot be represented as a convex combination of } \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{4}. \\\\\n",
    "\n",
    "&\\text{Thus, we have disproved the assertion with this counterexample.}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Gradient\n",
    "\n",
    "**Remark**: Assume all functions in this section are differentiable.\n",
    "\n",
    "\n",
    "**Directional Derivative**\n",
    "\n",
    "Let $f : \\mathbb{R}^{d} \\to \\mathbb{R}$ and let $\\boldsymbol{x}_{0} \\in \\mathbb{R}^{d}$. \n",
    "\n",
    "### 2.1. Question\n",
    "\n",
    "Prove that:\n",
    "\n",
    "$$ \\forall \\boldsymbol{h} \\in \\mathbb{R}^{d}: \\nabla f \\left( \\boldsymbol{x}_{0} \\right) \\left[ \\boldsymbol{h} \\right] = \\left\\langle \\boldsymbol{g}_{0}, \\boldsymbol{h} \\right\\rangle \\implies \\boldsymbol{g}_{0} = \\nabla f \\left( \\boldsymbol{x}_{0} \\right) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Solution\n",
    "\n",
    "\n",
    "The gradient is difined by all of its partial derivatives by direction:\n",
    "\n",
    "\n",
    "$\\nabla f \\left( \\boldsymbol{x}_{0} \\right)_i = \\frac {\\partial f(x_0)}{ \\partial x_i } $\n",
    "\n",
    "where $ \\frac {\\partial f(x_0)}{ \\partial x_i } $ is the partial derivative of $f$ with respect to variable $i$ evaluated at $𝑥_0$.\n",
    "\n",
    "Let $ e_i[j] = \\{1 $ if  $j=i$, else   $0 \\}$\n",
    "\n",
    "$ \\frac {\\partial f(x_0)}{ \\partial x_i } = \\lim_{t \\to 0} \\frac{f(x_0 + te_i) - f(x)}{t} = \\nabla f(x_0)[e_i] =  \\langle g_0, e_i\\rangle $ \n",
    "\n",
    "Such that for all partial derivatives:\n",
    "\n",
    "$ \\begin{bmatrix} \\frac {\\partial f(x_0)}{ \\partial x_1 }e_1 \\\\  \\frac {\\partial f(x_0)}{ \\partial x_2 }e_2  \\\\ \\vdots \\\\  \\frac {\\partial f(x_0)}{ \\partial x_i }e_i  \\end{bmatrix} =  \\langle \\nabla f \\left( \\boldsymbol{x}_{0} \\right)_i, e_i \\rangle =  \\langle g_0, e_i\\rangle $\n",
    "\n",
    "And therefore it holds that:\n",
    "\n",
    " $ \\nabla f(x_0) = g_0 $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**\n",
    "\n",
    "$f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ is said to be **linear** if:\n",
    "\n",
    "$$ f \\left( \\alpha \\boldsymbol{x} + \\beta \\boldsymbol{y} \\right) = \\alpha f \\left( \\boldsymbol{x} \\right) + \\beta f \\left( \\boldsymbol{y} \\right) $$\n",
    "\n",
    "For all $\\alpha, \\beta \\in \\mathbb{R}$ and for all $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{{d}_{1}}$.\n",
    "\n",
    "\n",
    "\n",
    "Let $f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ be a linear function.\n",
    "\n",
    "### 2.2. Question\n",
    "\n",
    "Prove that:\n",
    "\n",
    "$$ \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] = f \\left( \\boldsymbol{h} \\right) $$\n",
    "\n",
    "For all $\\boldsymbol{x}, \\boldsymbol{h} \\in \\mathbb{R}^{{d}_{1}}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Solution\n",
    "\n",
    "By definition:\n",
    "\n",
    "$ \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] = \\lim_{t \\to 0} \\frac{f(x + th) - f(x)} {t} $\n",
    "\n",
    "By the linear definition we get:\n",
    "\n",
    "$ \\lim_{t \\to 0} \\frac{f(x + th) - f(x)} {t} = \\lim_{t \\to 0} \\frac{f(x) + f(th) - f(x)} {t} =  \\lim_{t \\to 0} \\frac{ f(th)} {t} =  \\lim_{t \\to 0} \\frac{ tf(h)} {t} = f(h)$\n",
    "                  \n",
    "\n",
    "Therefore $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] = f(h)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Solution\n",
    "\n",
    "$\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]= \\nabla \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}\\left[ \\boldsymbol{h} \\right] = \\nabla \\left\\langle \\boldsymbol{x}, \\boldsymbol{A} \\boldsymbol{x}\\right\\rangle \\left[ \\boldsymbol{h} \\right]$\\\n",
    "By product rule = $ \\left\\langle \\nabla\\boldsymbol{x}\\left[ \\boldsymbol{h} \\right], \\boldsymbol{A} \\boldsymbol{x}\\right\\rangle + \\left\\langle \\boldsymbol{x}, \\nabla\\boldsymbol{A} \\boldsymbol{x}\\left[ \\boldsymbol{h} \\right]\\right\\rangle$\\\n",
    "By linearity = $ \\left\\langle \\boldsymbol{h}, \\boldsymbol{A} \\boldsymbol{x}\\right\\rangle + \\left\\langle \\boldsymbol{x}, \\boldsymbol{A} \\boldsymbol{h}\\right\\rangle = \\left\\langle \\boldsymbol{h}, \\boldsymbol{A} \\boldsymbol{x}\\right\\rangle + \\left\\langle \\boldsymbol{A}^\\top\\boldsymbol{x}, \\boldsymbol{h}\\right\\rangle  = \\left\\langle  \\boldsymbol{A} \\boldsymbol{x},\\boldsymbol{h}\\right\\rangle + \\left\\langle \\boldsymbol{A}^\\top\\boldsymbol{x}, \\boldsymbol{h}\\right\\rangle = \\left\\langle (\\boldsymbol{A} +\\boldsymbol{A}^\\top)\\boldsymbol{x}, \\boldsymbol{h}\\right\\rangle$\\\n",
    "\n",
    "$\\implies  \\nabla f \\left( \\boldsymbol{x} \\right) = (\\boldsymbol{A} +\\boldsymbol{A}^\\top)\\boldsymbol{x}$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( X \\right) \\left[ \\boldsymbol{H} \\right]$ and the gradient $\\nabla f \\left( X \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{A} \\boldsymbol{X} \\right\\} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Solution\n",
    "\n",
    "If we use the inner product:\n",
    "\n",
    " $\\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{A} \\boldsymbol{X} \\right\\} = \\langle\\ \\boldsymbol{X},  \\boldsymbol{A} \\boldsymbol{X} \\rangle\\ $\n",
    "\n",
    " As we saw before:\n",
    "\n",
    " Via product rule = $ \\left\\langle \\nabla\\boldsymbol{X}\\left[ \\boldsymbol{H} \\right], \\boldsymbol{A} \\boldsymbol{X}\\right\\rangle + \\left\\langle \\boldsymbol{X}, \\nabla\\boldsymbol{A} \\boldsymbol{X}\\left[ \\boldsymbol{H} \\right]\\right\\rangle$\n",
    "\n",
    " As we saw before deriving by a matrix $\\boldsymbol{X}$, would get us to the similar result of:\\\n",
    "$\\nabla f \\left( \\boldsymbol{X} \\right) \\left[ \\boldsymbol{H} \\right]= \\nabla \\left\\langle \\boldsymbol{X}, \\boldsymbol{A} \\boldsymbol{X}\\right\\rangle \\left[ \\boldsymbol{H} \\right]$\\\n",
    "By product rule = $ \\left\\langle \\nabla\\boldsymbol{X}\\left[ \\boldsymbol{H} \\right], \\boldsymbol{A} \\boldsymbol{X}\\right\\rangle + \\left\\langle \\boldsymbol{X}, \\nabla\\boldsymbol{A} \\boldsymbol{X}\\left[ \\boldsymbol{H} \\right]\\right\\rangle$\\\n",
    "By linearity = $ \\left\\langle \\boldsymbol{H}, \\boldsymbol{A} \\boldsymbol{X}\\right\\rangle + \\left\\langle \\boldsymbol{X}, \\boldsymbol{A} \\boldsymbol{H}\\right\\rangle = \\left\\langle \\boldsymbol{H}, \\boldsymbol{A} \\boldsymbol{X}\\right\\rangle + \\left\\langle \\boldsymbol{A}^\\top\\boldsymbol{X}, \\boldsymbol{H}\\right\\rangle  = \\left\\langle  \\boldsymbol{A} \\boldsymbol{X},\\boldsymbol{H}\\right\\rangle + \\left\\langle \\boldsymbol{A}^\\top\\boldsymbol{X}, \\boldsymbol{H}\\right\\rangle = \\left\\langle (\\boldsymbol{A} +\\boldsymbol{A}^\\top)\\boldsymbol{X}, \\boldsymbol{H}\\right\\rangle$\\\n",
    "$\\implies  \\nabla f \\left( \\boldsymbol{X} \\right) = (\\boldsymbol{A} +\\boldsymbol{A}^\\top)\\boldsymbol{X}$\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = {\\left\\| \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x} \\right\\|}_{2}^{2} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Solution\n",
    "\n",
    "Assume $w\\left(\\boldsymbol{z}\\right)={\\left\\Vert \\boldsymbol{z}\\right\\Vert }_{2}^{2} \\Longrightarrow\\nabla w\\left(\\boldsymbol{z}\\right)=2\\boldsymbol{z}$\n",
    "\n",
    "Assume $g\\left(\\boldsymbol{x}\\right)=\\boldsymbol{y}-\\boldsymbol{A}\\boldsymbol{x} \\Longrightarrow\\nabla g\\left(x\\right)\\left[\\boldsymbol{h}\\right]=-\\boldsymbol{Ah}$\n",
    "\n",
    "$f\\left(\\boldsymbol{x}\\right)=(w\\circ g\\left)(\\boldsymbol{x}\\right)$\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$\\nabla ((w\\circ g\\left)(\\boldsymbol{x}\\right))\\left[\\boldsymbol{h}\\right]=\\langle w\\left(\\boldsymbol{g\\left(\\boldsymbol{x}\\right)}\\right),\\nabla g\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]\\rangle$\n",
    "\n",
    "$\\nabla f\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]=\\nabla ((w\\circ g\\left)(\\boldsymbol{x}\\right))\\left[\\boldsymbol{h}\\right]=\\langle w\\left(g\\left(\\boldsymbol{x}\\right)\\right),\\nabla g\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]\\rangle$\n",
    "\n",
    "$\\Longrightarrow\\langle2g\\left(\\boldsymbol{x}\\right),-\\boldsymbol{Ah}\\rangle=\\langle2\\left(\\boldsymbol{y}-\\boldsymbol{A}\\boldsymbol{x}\\right),-\\boldsymbol{Ah}\\rangle=\\langle-2A^{T}\\left(\\boldsymbol{y}-\\boldsymbol{A}\\boldsymbol{x}\\right),\\boldsymbol{h}\\rangle$\n",
    "\n",
    "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{x}\\right)=-2A^{T}\\left(\\boldsymbol{y}-\\boldsymbol{A}\\boldsymbol{x}\\right)$\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( X \\right) \\left[ H \\right]$ and the gradient $\\nabla f \\left( X \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = {\\left\\| \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X} \\right\\|}_{F}^{2} $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{D \\times d}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$.\n",
    " - ${\\left\\| \\cdot \\right\\|}_{F}^{2}$ is the squared [Frobenius Norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm), that is, ${\\left\\| \\boldsymbol{X} \\right\\|}_{F}^{2} = \\left\\langle \\boldsymbol{X}, \\boldsymbol{X} \\right\\rangle = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{X} \\right\\}$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Solution\n",
    "\n",
    "\n",
    "Assume $w\\left(\\boldsymbol{Z}\\right)={\\left\\Vert \\boldsymbol{Z}\\right\\Vert }_{F}^{2} \\Longrightarrow\\nabla w\\left(\\boldsymbol{Z}\\right)=2\\boldsymbol{Z}$\n",
    "\n",
    "Assume $g\\left(X\\right)=\\boldsymbol{Y}-\\boldsymbol{A}\\boldsymbol{x} \\Longrightarrow\\nabla g\\left(x\\right)\\left[\\boldsymbol{H}\\right]=-\\boldsymbol{AH}$\n",
    "\n",
    "$f\\left(\\boldsymbol{X}\\right)=w\\circ g\\left(\\boldsymbol{X}\\right)$\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$\\nabla\\langle w\\left(\\boldsymbol{X}\\right)\\circ g\\left(\\boldsymbol{X}\\right)\\rangle\\left[H\\right]=\\langle w\\left(\\boldsymbol{g\\left(\\boldsymbol{X}\\right)}\\right),\\nabla g\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{H}\\right]\\rangle$\n",
    "\n",
    "$\\nabla f\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{H}\\right]=\\nabla\\langle w\\circ g\\left(\\boldsymbol{X}\\right)\\rangle\\left[\\boldsymbol{H}\\right]=\\langle w\\left(g\\left(\\boldsymbol{X}\\right)\\right),\\nabla g\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{H}\\right]\\rangle$\n",
    "\n",
    "$\\Longrightarrow\\langle2g\\left(\\boldsymbol{X}\\right),-\\boldsymbol{AH}\\rangle=\\langle2\\left(\\boldsymbol{Y}-\\boldsymbol{A}\\boldsymbol{X}\\right),-\\boldsymbol{AH}\\rangle=\\langle-2A^{T}\\left(\\boldsymbol{Y}-\\boldsymbol{A}\\boldsymbol{X}\\right),\\boldsymbol{H}\\rangle$\n",
    "\n",
    "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{X}\\right)=-2A^{T}\\left(\\boldsymbol{Y}-\\boldsymbol{A}\\boldsymbol{X}\\right)$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( X \\right) \\left[ H \\right]$ and the gradient $\\nabla f \\left( X \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{X}^{T} \\boldsymbol{A}, \\boldsymbol{Y}^{T} \\right\\rangle $$\n",
    "\n",
    "Where $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times D}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Solution\n",
    "\n",
    "$ f \\left( \\boldsymbol{X} \\right) =  \\left\\langle \\boldsymbol{X}^{\\top} \\boldsymbol{A}, \\boldsymbol{Y}^{\\top} \\right\\rangle$\n",
    "\n",
    "$\\nabla f \\left( \\boldsymbol{X} \\right) \\left[ \\boldsymbol{H} \\right]= \\nabla\\left\\langle \\boldsymbol{X}^{\\top} \\boldsymbol{A}, \\boldsymbol{Y}^{\\top} \\right\\rangle\\left[ \\boldsymbol{H} \\right]$\\\n",
    "By product rule = $ \\left\\langle \\nabla\\boldsymbol{X}^{\\top} \\boldsymbol{A}\\left[ \\boldsymbol{H} \\right], \\boldsymbol{Y}^{\\top} \\right\\rangle + \\left\\langle \\boldsymbol{X}^{\\top} \\boldsymbol{A}, \\nabla\\boldsymbol{Y}^{\\top}\\left[ \\boldsymbol{H} \\right] \\right\\rangle$\n",
    "\n",
    "By linearity = $ \\left\\langle \\boldsymbol{H}^{\\top} \\boldsymbol{A}, \\boldsymbol{Y}^{\\top} \\right\\rangle = \\operatorname{Tr} \\left\\{\\boldsymbol{A}^\\top\\boldsymbol{H}\\boldsymbol{Y}^\\top\\right\\} =  \\operatorname{Tr} \\left\\{\\boldsymbol{Y}^\\top\\boldsymbol{A}^\\top\\boldsymbol{H}\\right\\} = \\left\\langle  \\boldsymbol{A}\\boldsymbol{Y}, \\boldsymbol{H} \\right\\rangle$\n",
    "\n",
    "$\\implies  \\nabla f \\left( \\boldsymbol{X} \\right) = \\boldsymbol{A}\\boldsymbol{Y}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = {a}^{T} g \\left( \\boldsymbol{x} \\right) $$\n",
    "\n",
    "Where $g \\left( \\cdot \\right)$ is an element wise function $g \\left( \\boldsymbol{x} \\right) = \\begin{bmatrix} g \\left( {x}_{1} \\right) \\\\ g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} \\right) \\end{bmatrix} \\in \\mathbb{R}^{d}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Solution\n",
    "\n",
    "\n",
    "$f \\left( \\boldsymbol{x} \\right) = {a}^{T} g \\left( \\boldsymbol{x} \\right) = \\sum_{i=1}^d a_ig(x_i)$\\\n",
    "$\\implies \\frac{\\partial f}{\\partial x_i} = a_i\\acute{g}(x_i)$\\\n",
    "$\\implies \\nabla f \\left( \\boldsymbol{x} \\right) = \\begin{bmatrix}a_1 \\acute g \\left( {x}_{1} \\right) \\\\a_2 \\acute g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\a_d \\acute g \\left( {x}_{d} \\right) \\end{bmatrix} = \\boldsymbol{a} \\circ\\nabla g\\left( \\boldsymbol{x} \\right)\\quad$ \\\n",
    "$\\implies \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] = \\left\\langle \\boldsymbol{a} \\circ\\nabla g\\left( \\boldsymbol{x} \\right), \\boldsymbol{h}\\right\\rangle$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( X \\right) \\left[ H \\right]$ and the gradient $\\nabla f \\left( X \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{a}, \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\right\\rangle $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
    " - The function $\\operatorname{Diag} \\left( \\cdot \\right) : \\mathbb{R}^{d \\times d} \\to \\mathbb{R}^{d} $ returns the diagonal of a matrix, that is, $\\boldsymbol{b} = \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{b} \\left[ i \\right] = \\left( \\boldsymbol{X} \\left[ i, i\\right] \\right)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. Solution\n",
    "\n",
    "Let $Diag  \\left( \\cdot \\right) : \\mathbb{R}^{d} \\to \\mathbb{R}^{d \\times d} $ returns the diagonal matrix of a vector, that is, $\\boldsymbol{b} = Diag \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{b} \\left[ i ,i \\right] = \\boldsymbol{x} \\left[ i\\right]$\n",
    "\n",
    "From the diagonal property, it holds that:\n",
    "\n",
    "$f(X) = \\langle a, diag(X) \\rangle =   \\langle Diag(a), X\\rangle$\n",
    "\n",
    "Since f is linear:\n",
    "\n",
    "\n",
    "$\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]  = f(h) = \\langle DIAG(a), h\\rangle$\n",
    "\n",
    "and therefore the gradient $\\nabla f \\left( \\boldsymbol{x} \\right) = Diag(a)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( X \\right) \\left[ H \\right]$ and the gradient $\\nabla f \\left( X \\right)$ of:\n",
    "\n",
    "$$ f \\left( X \\right) = \\left \\langle A, \\sin \\left[ X \\right] \\right \\rangle $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
    " - The function $\\sin \\left[ \\cdot \\right]$ is the element wise $\\log$ function: $\\boldsymbol{M} = \\sin \\left[ \\boldsymbol{X} \\right] \\implies \\boldsymbol{M} \\left[ i, j \\right] = \\sin \\left( \\boldsymbol{X} \\left[ i, j\\right] \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11. Solution\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\text{Given:} \\quad f \\left( X \\right) = \\left \\langle A, \\sin \\left[ X \\right] \\right \\rangle \\\\\n",
    "\n",
    "&\\text{1. Gradient of } f: \\\\\n",
    "&\\frac{\\partial \\sin[X]}{\\partial X} = \\cos[X] \\\\\n",
    "&\\nabla f(X) = A \\odot \\cos[X] \\\\\n",
    "\n",
    "&\\text{2. Directional Derivative:} \\\\\n",
    "&\\nabla f(X)[H] = \\langle \\nabla f(X), H \\rangle_F \\\\\n",
    "\n",
    "&\\text{Using our expression for the gradient:} \\\\\n",
    "&\\nabla f(X)[H] = \\langle A \\odot \\cos[X], H \\rangle_F \\\\\n",
    "\n",
    "&\\text{In summary:} \\\\\n",
    "&1. \\text{The gradient of } f \\text{ is:} \\\\\n",
    "&\\nabla f(X) = A \\odot \\cos[X] \\\\\n",
    "&2. \\text{The directional derivative of } f \\text{ in the direction of } H \\text{ is:} \\\\\n",
    "&\\nabla f(X)[H] = \\langle A \\odot \\cos[X], H \\rangle_F \\\\\n",
    "\\end{aligned}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constraint Optimization\n",
    "\n",
    "**MinMax**  \n",
    "\n",
    "Let $G \\left( x, y \\right) = \\sin \\left( x + y \\right)$.\n",
    "\n",
    "### 3.1. Question\n",
    "\n",
    "Show that:\n",
    "\n",
    " - $\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = 1$.\n",
    " - $\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = -1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Solution\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\text{Given } G \\left( x, y \\right) = \\sin \\left( x + y \\right), \\\\\n",
    "&1. \\text{For the inner maximization:} \\\\\n",
    "&\\quad \\frac{\\partial G}{\\partial y} = \\cos(x + y), \\\\\n",
    "&\\quad \\underset{y}{\\max} G \\left( x, y \\right) = 1 \\text{ for all } x. \\\\\n",
    "&2. \\text{For the outer minimization:} \\\\\n",
    "&\\quad \\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = 1, \\\\\n",
    "&3. \\text{For the inner minimization:} \\\\\n",
    "&\\quad \\underset{x}{\\min} G \\left( x, y \\right) = -1 \\text{ for all } y. \\\\\n",
    "&4. \\text{For the outer maximization:} \\\\\n",
    "&\\quad \\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = -1. \\\\\n",
    "\\end{aligned}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rayleigh Quotient**  \n",
    "\n",
    "The _Rayleigh Quotient_ is defined by:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = \\frac{ \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} }{ \\boldsymbol{x}^{T} \\boldsymbol{x}} $$\n",
    "\n",
    "For some symmetric matrix $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$.\n",
    "\n",
    "### 3.2. Question\n",
    "\n",
    "For a symmetric matrix $ A $ which has non negative eigen values, Follow the given steps:\n",
    "\n",
    " - Show that $ {\\min}_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right) = \\begin{cases} {\\min}_{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} \\\\ \\text{ s.t. } {\\left\\| \\boldsymbol{x} \\right\\|}_{2}^{2} = 1 \\end{cases} $.\n",
    " - Write the Lagrangian of the constraint objective $\\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right)$.\n",
    " - Show that ${\\nabla}_{\\boldsymbol{x}} \\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right) = 0 \\iff \\boldsymbol{A} \\boldsymbol{x} = \\lambda \\boldsymbol{x}$.  \n",
    "   In other words, the stationary points $\\left( \\boldsymbol{x}, \\lambda \\right)$ are the eigenvectors and eigenvalues of $\\boldsymbol{A}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Solution\n",
    "\n",
    " \n",
    "To find the min we need to finds $\\nabla f \\left( \\boldsymbol{x} \\right) = 0$:\n",
    "$$\\nabla f \\left( \\boldsymbol{x} \\right) = ∇_x  \\frac{ \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} }{ \\boldsymbol{x}^{T} \\boldsymbol{x}} $$\n",
    "Hense:\n",
    "$$\\nabla_x\\frac{\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} }{ \\boldsymbol{x}^{T} \\boldsymbol{x}} = \\frac{ \\nabla(\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x})\\boldsymbol{x}^{T} \\boldsymbol{x} - \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} \\nabla(\\boldsymbol{x}^{T} \\boldsymbol{x})}{( \\boldsymbol{x}^{T} \\boldsymbol{x})^2}$$\n",
    "We proved in 2.3 that $\\nabla(\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x})=\\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{A}^T\\boldsymbol{x} = 2\\boldsymbol{A}\\boldsymbol{x}$ since $\\boldsymbol{A}$ is symmetric, and $\\nabla(\\boldsymbol{x}^{T} \\boldsymbol{x}) = 2\\boldsymbol{x}$.\n",
    " So:\n",
    "\n",
    "$$\\nabla f \\left( \\boldsymbol{x} \\right) = \\frac{ 2(\\boldsymbol{A}\\boldsymbol{x}) \\boldsymbol{x}^{T} \\boldsymbol{x} - 2(\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}) \\boldsymbol{x}}{( \\boldsymbol{x}^{T} \\boldsymbol{x})^2}. $$\n",
    "To find   $\\nabla f \\left( \\boldsymbol{x} \\right) = 0$.  \n",
    "$$\\nabla f \\left( \\boldsymbol{x} \\right) = 0 \\iff \\frac{ 2(\\boldsymbol{A}\\boldsymbol{x}) \\boldsymbol{x}^{T} \\boldsymbol{x} - 2(\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}) \\boldsymbol{x}}{( \\boldsymbol{x}^{T} \\boldsymbol{x})^2} = 0 \\iff 2(\\boldsymbol{A}\\boldsymbol{x}) \\boldsymbol{x}^{T} \\boldsymbol{x} - 2(\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}) \\boldsymbol{x} = 0 $$ \n",
    "$$\\iff (\\boldsymbol{A}\\boldsymbol{x}) \\boldsymbol{x}^{T} \\boldsymbol{x} - (\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}) \\boldsymbol{x} = 0 \\iff \\boldsymbol{A} \\boldsymbol{x} = \\frac {\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}}{\\boldsymbol{x}^{T} \\boldsymbol{x}}\\boldsymbol{x} = f(\\boldsymbol{x})\\boldsymbol{x}$$\n",
    "\n",
    "\n",
    "It can be deduced that for a matrix $\\boldsymbol{A}$, any eigenvector $\\boldsymbol{x}$ associated with the eigenvalue $f(\\boldsymbol{x})$ fulfills the given equation. Specifically, the vectors $\\boldsymbol{x}$ that reduce $f \\left( \\boldsymbol{x} \\right)$ to its smallest value are indeed the eigenvectors of $\\boldsymbol{A}$. The corresponding eigenvalue for each eigenvector $\\boldsymbol{x}$ is $f(\\boldsymbol{x})$. Thus, our goal becomes identifying the eigenvector related to the smallest eigenvalue. Given that eigenvalues can be normalized, the eigenvector can be too, so that ${\\left| \\boldsymbol{x} \\right|}_{2}^{2} = 1$. Consequently, we deduce: ${\\min}_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right)= {\\min}_{\\boldsymbol{x}}\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/qIP5xPv.png\" height=\"700\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

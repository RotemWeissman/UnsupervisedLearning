{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 002 - Part I\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 16/08/2023 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_08/Exercise0002Part001.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
    " - Answer all questions / tasks within the Jupyter Notebook.\n",
    " - Use MarkDown + MathJaX + Code to answer.\n",
    " - Verify the rendering on VS Code.\n",
    " - Submission in groups (Single submission per group).\n",
    " - You may and _should_ use the forums for questions.\n",
    " - Good Luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "\n",
    " - `<Ilan>_<Schaedel>_<011708005>`.\n",
    " - `<Barak>_<Ben Menachem>_<312527229>`.\n",
    " - `<Rotem>_<Weissman>_<203958103>`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Algebra\n",
    "\n",
    "The Frobenius Norm ${\\left\\| \\cdot \\right\\|}_{F} : \\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{+}$ is defined as ${\\left\\| A \\right\\|}_{F} = \\sqrt{\\sum_{i} \\sum_{j} {A}_{ij}^{2}}$.\n",
    "\n",
    "### 1.1. Question\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$ and ${\\lambda}_{i} \\left( M \\right)$ is the $i$ -th eigen value of the matrix $M$ (Assuming $M$ has a valid eigen decomposition).  \n",
    "Prove that ${\\left\\| A \\right\\|}_{F}^{2} = \\sum_{i = 1}^{n} {\\lambda}_{i} \\left( {A}^{T} A \\right)$.  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> Make sure to show why ${\\lambda}_{i} \\left( {A}^{T} A \\right)$ exists."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Solution\n",
    "\n",
    "$A^TA$ is symmetric and real and diagonalizable, since $A^TA=(A^TA)^T$. So it has $n$ real eigenvalues and eigenvectors by the spectral theorem.\n",
    "We can write $A^TA$ as:\n",
    "\n",
    "$VDV^T$ where $V$ is the matrix whose columns are $v_1,v_2,...,v_n$ (the eigenvectors of $A^TA$) and $D$ is the diagonal matrix with $\\lambda_1(A^TA),\\lambda_2(A^TA),...,\\lambda_n(A^TA)$ (the eigenvalues) on the diagonal.\n",
    "\n",
    "$V^TV = I$ since the eigenvectors are orthogonal.\n",
    "\n",
    "so,\n",
    "\n",
    "$||A||_F^2 = \\sum_i\\sum_jA_{ij}^2 = Tr(A^TA) = Tr(VDV^T) = Tr(DV^TV) = Tr(D) = \\sum_{i=1}^n\\lambda_i(A^TA)$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix $Q \\in \\mathbb{R}^{n \\times n}$ is called a positive definite matrix if and only if $\\forall \\boldsymbol{x} \\in \\mathbb{R}^{n} \\setminus \\left\\{ \\boldsymbol{0} \\right\\}, \\; \\boldsymbol{x}^{T} Q \\boldsymbol{x} > \\boldsymbol{0}$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The following are the common notations:\n",
    "    * $\\boldsymbol{S}^{N}      = \\left\\{ X \\in \\mathbb{R}^{n \\times n} \\mid X = {X}^{T} \\right\\}$ (Symmetric matrices).\n",
    "    * $\\boldsymbol{S}^{N}_{+}  = \\left\\{ X \\in \\mathbb{R}^{n \\times n} \\mid X \\succeq 0 \\right\\}$ (Positive semi definite matrices which are symmetric).\n",
    "    * $\\boldsymbol{S}^{N}_{++} = \\left\\{ X \\in \\mathbb{R}^{n \\times n} \\mid X \\succ 0 \\right\\}$ (Positive definite matrices which are symmetric).\n",
    "\n",
    "### 1.2. Question\n",
    "\n",
    "Let $Q \\in \\mathbb{R}^{n \\times n}$ be a positive definite matrix. Show that ${\\left\\| \\boldsymbol{x} \\right\\|}_{Q} = \\sqrt{\\boldsymbol{x}^{T} Q \\boldsymbol{x}}$ is a norm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Solution\n",
    "\n",
    "We'll show that $||x||_Q$ satisfies the four properties of a norm:\n",
    "\n",
    "1. Positive Definite: ($||x|| = 0 \\iff x = \\boldsymbol{0}$)\n",
    "\n",
    "By definition, $x^TQx > \\boldsymbol{0} ,\\forall x\\in\\mathbb{R}^n \\setminus \\{\\boldsymbol{0}\\}$\n",
    "\n",
    "So,\n",
    "\n",
    "$\\sqrt{x^TQx} > 0 ,\\forall x\\in\\mathbb{R}^n \\setminus \\{\\boldsymbol{0}\\}$\n",
    "\n",
    "For $x=\\boldsymbol{0}$:\n",
    "\n",
    "$x^TQx=0$, hence the property holds.\n",
    "\n",
    "2. We also showed here that $\\sqrt{x^TQx} \\geq 0 ,\\forall x\\in\\mathbb{R}^n$ (Non negetivity)\n",
    "\n",
    "3. Absolutely Homogeneous: ($||\\alpha x|| = |\\alpha|\\cdot||x||$)\n",
    "\n",
    "$$‚àÄx\\ ‚àà\\ R^{n},\\ c\\ ‚àà\\ R\\ \\ \\left|\\left|cx\\right|\\right|_{Q}=\\sqrt{\\left(cx\\right)^{T}Q\\left(cx\\right)}=\\sqrt{c^{2}x^{T}Qx}=\\left|c\\right|\\sqrt{x^{T}Qx}=\\left|c\\right|\\left|\\left|x\\right|\\right|_{Q}$$\n",
    "\n",
    "4. Triangle Inequality: ($||x+y||\\leq ||x||+||y||$)\n",
    "\n",
    "$$||x+y||_Q \\leq ||x||_Q + ||y||_Q$$\n",
    "\n",
    "$$\\sqrt{(x+y)^TQ(x+y)} \\leq \\sqrt{x^TQx} + \\sqrt{y^TQy}$$\n",
    "\n",
    "$$(x+y)^TQ(x+y) \\leq x^TQx + y^TQy + 2\\sqrt{x^TQx}\\sqrt{y^TQy} = x^TQx + y^TQy + 2\\sqrt{x^TQx\\cdot y^TQy}$$\n",
    "\n",
    "$$x^TQx + x^TQy + y^TQx + y^TQy \\leq x^TQx + y^TQy + 2\\sqrt{x^TQx\\cdot y^TQy}$$\n",
    "\n",
    "$$y^TQx + x^TQy \\leq 2\\sqrt{x^TQx\\cdot y^TQy}$$\n",
    "\n",
    "$$2x^TQy \\leq 2\\sqrt{x^TQx\\cdot y^TQy}$$\n",
    "\n",
    "\n",
    "This expresion is true by Cauchy-Schwarz inequality.\n",
    "\n",
    "Since the function satisfy all the properties of a norm, it is a norm.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix $U \\in \\mathbb{R}^{n \\times n}$ is called an _orthogonal matrix_ if and only if ${U}^{T} U = U {U}^{T} = {U}^{-1} U = U {U}^{-1} = I$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> For matrices over $\\mathbb{C}$ we call such matrices a unitary matrices.\n",
    "\n",
    "### 1.3. Question\n",
    "\n",
    "Show that for any orthogonal matrix $U$ is an isometry with respect to the euclidean norm, that is ${\\left\\| U \\boldsymbol{x} \\right\\|}_{2} = {\\left\\| x \\right\\|}_{2}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Solution\n",
    "\n",
    "$||Ux||_2 = (Ux)^T(Ux) = x^TU^TUx = x^TIx = ||x||_2$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $R = \\begin{bmatrix*}[r] \\cos \\left( \\theta \\right) & - \\sin \\left( \\theta \\right) \\\\ \\sin \\left( \\theta \\right) & \\cos \\left( \\theta \\right) \\end{bmatrix*}$ is called a rotation matrix.\n",
    "\n",
    "### 1.4. Question\n",
    "\n",
    "For the set of matrices of size $2 \\times 2$, Prove or disprove:\n",
    "\n",
    " - The matrix $R$ is a orthogonal matrix.\n",
    " - Any orthogonal matrix is a rotation matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Solution\n",
    "\n",
    "the inverse of a $ 2 x 2$ matrix $A=\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$ is given by $A^{-1}=\\frac{1}{ad-bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}$ <br> <br>\n",
    "    Also note, that by basic geometric properties $\\cos^{2}\\left(\\theta \\right)+\\sin^{2}\\left(\\theta \\right)=1$. So: \n",
    "\n",
    "$$R^{-1} = \\frac{1}{\\cos^2(\\theta)+\\sin^2(\\theta)}\\begin{bmatrix}\\cos(\\theta) & \\sin(\\theta) \\\\ -\\sin(\\theta) & \\cos(\\theta)\\end{bmatrix} = \\begin{bmatrix}\\cos(\\theta) & \\sin(\\theta) \\\\ -\\sin(\\theta) & \\cos(\\theta)\\end{bmatrix} = R^T$$\n",
    "\n",
    "$$\\implies R^TR = R^{-1}R = I$$\n",
    "$$RR^T = RR^{-1} = I$$\n",
    "\n",
    "$R$ is an orthogonal matrix, the first statment is true.\n",
    "\n",
    "The second statment is false. Consider the following counter example:\n",
    "\n",
    "$A = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}$\n",
    "\n",
    "$A$ is an orthogonal matrix since: $A^{-1} = \\frac{1}{-1}\\begin{bmatrix} -1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} = A^T = A$\n",
    "\n",
    "But, $Det(A) = -1$ in oppose to the property of a rotation matrix, for which the determinant is always 1.\n",
    "\n",
    "Hence, $A$ is not a rotation matrix, eventhough it is an orthogonal matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimization\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Question\n",
    "\n",
    "Find the global minimum and maximum points of the linear function $f \\left( x, y \\right) = 7 x + 12 y$ over the set $\\mathcal{S} = \\left\\{ \\left( x, y \\right) \\mid 2 {x}^{2} + 6 x y + 9 {y}^{2} - 2 x - 6 y \\leq 24 \\right\\}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Solution\n",
    "\n",
    "$\\mathcal{L}(x,y,\\lambda) = 7x + 12y - \\lambda(2x^2 + 6xy + 9y^2 - 2x - 6y - 24)$\n",
    "\n",
    "$\\nabla_x\\mathcal{L}(x,y,\\lambda) = 7 - \\lambda(4x + 6y - 2)$\n",
    "\n",
    "$\\nabla_y\\mathcal{L}(x,y,\\lambda) = 12 - \\lambda(6x + 18y - 6)$\n",
    "\n",
    "$\\nabla_{\\lambda}\\mathcal{L}(x,y,\\lambda) = - (2x^2 + 6xy + 9y^2 - 2x - 6y - 24)$\n",
    "\n",
    "$\\nabla\\mathcal{L}(x,y,\\lambda) = \\begin{bmatrix} 7 - \\lambda(4x + 6y - 2) \\\\ 12 - \\lambda(6x + 18y - 6) \\\\ - (2x^2 + 6xy + 9y^2 - 2x - 6y - 24) \\end{bmatrix}$\n",
    "\n",
    "Setting to zero we get the system of equations:\n",
    "\n",
    "$\\begin{equation}\\begin{cases} 7 - \\lambda(4x + 6y - 2) = 0 \\\\ 12 - \\lambda(6x + 18y - 6) = 0 \\\\ - (2x^2 + 6xy + 9y^2 - 2x - 6y - 24) = 0 \\end{cases}\\end{equation}$\n",
    "\n",
    "We solve the equations to get:\n",
    "\n",
    "$x = \\frac{3}{2\\lambda}$, $y=\\frac{1+2\\lambda}{6\\lambda}$, $\\lambda=\\mp0.5$\n",
    "\n",
    "$\\implies x = \\mp 3, y=0, \\frac{2}{3}$ and we get:\n",
    "\n",
    "$min f(x,y) = f(-3,0) = -21$\n",
    "\n",
    "$max f(x,y) = f(3,\\frac{2}{3}) = 29$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Means\n",
    "\n",
    "The K-Means objective is given by:\n",
    "\n",
    "$$ \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$\n",
    "\n",
    "### 3.1. Question\n",
    "\n",
    "Show that the following 2 objectives are equivalent to the K-Means objectives:\n",
    "\n",
    "1. As a function of the clusters:\n",
    "\n",
    "$$ \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\} } \\sum_{k = 1}^{K} \\frac{1}{\\left| \\mathcal{D}_{k} \\right|} \\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} $$\n",
    "\n",
    "2. As a function of the centroids:\n",
    "\n",
    "$$ \\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{i = 1}^{N} \\min_{k} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Solution\n",
    "\n",
    "1. For the first statement:\n",
    "\n",
    "We can open the argument to be:\n",
    "$$ \\sum_{x_i,x_j\\in D_k}||x_i-x_j||^2 = \\sum_{x_i\\in D_k}\\sum_{x_j\\in D_k}\\left[||x_i||^2 + ||x_j||^2 - 2\\langle x_i,x_j\\rangle\\right]$$\n",
    "\n",
    "$$ = \\sum_{x_i\\in D_k}\\sum_{x_j\\in D_k}||x_i||^2 + \\sum_{x_i\\in D_k}\\sum_{x_j\\in D_k}||x_j||^2 - 2 \\sum_{x_i\\in D_k}\\sum_{x_j\\in D_k}\\langle x_i,x_j\\rangle$$\n",
    "\n",
    "$$ = \\sum_{x_i\\in D_k}|D_k|\\cdot||x_i||^2 + \\sum_{x_j\\in D_k}|D_k|\\cdot||x_j||^2 - 2|D_k|^2\\cdot||\\mu_k||^2 $$\n",
    "\n",
    "$$ = 2|D_k|\\left[\\sum_{x_i\\in D_k}||x_i||^2 - |D_k|\\cdot||\\mu_k||^2\\right] $$\n",
    "\n",
    "We can also simplify the K-Means objective to be of the same form, up to a scalar multiplication:\n",
    "\n",
    "$$\\sum_{x_i}||x_i-\\mu_k||^2 = \\sum_{x_i\\in D_k}\\left[||x_i||^2 + ||\\mu_k||^2 - 2\\langle x_i,\\mu_k\\rangle\\right]$$\n",
    "\n",
    "$$ = \\sum_{x_i\\in D_k}||x_i||^2 + \\sum_{x_i\\in D_k}||\\mu_k||^2 -2\\mu_k^T\\sum_{x_i\\in D_k}x_i $$\n",
    "\n",
    "$$ = \\sum_{x_i\\in D_k}||x_i||^2 + |D_k|\\cdot||\\mu_k||^2 - 2|D_k|\\cdot||\\mu_k||^2 $$\n",
    "\n",
    "$$ = \\sum_{x_i\\in D_k}||x_i||^2 - |D_k|\\cdot||\\mu_k||^2 $$\n",
    "\n",
    "So we got:\n",
    "\n",
    "$$ \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2}=\\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} (\\sum_{x_{i}‚ààD_{k}}^{ }\\left|\\left|x_{i}\\right|\\right|^{2}-\\left|D_{k}\\right|\\left|\\left|\\mu_{k}\\right|\\right|^{2}) $$\n",
    "\n",
    "$$  \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\} } \\sum_{k = 1}^{K} \\frac{1}{\\left| \\mathcal{D}_{k} \\right|} \\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} =  \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} 2(\\sum_{x_{i}‚ààD_{k}}^{ }\\left|\\left|x_{i}\\right|\\right|^{2}-\\left|D_{k}\\right|\\left|\\left|\\mu_{k}\\right|\\right|^{2})$$\n",
    "\n",
    "Hence the arguments are equivalent up to a scalar multiplication that doesn't effect minimazation.\n",
    "\n",
    "2. For the second statement:\n",
    "\n",
    "$$ \\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{i = 1}^{N} \\min_{k} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$\n",
    "\n",
    "Is equivalet to say:\n",
    "\n",
    "$$ \\argmin_{\\mu_k}\\sum_{k=1}^K\\sum_{i=1}^N[x_i\\in D_k]\\cdot||x_i - \\mu_k||_2^2 $$\n",
    "\n",
    "Where $ [x_i\\in D_k] $ is an indicator that recives 1 iff the condition is true and 0 otherwise. This way we sum up only the cases where the condition is met.\n",
    "\n",
    "For k-Means minimazation task, $x_i$ will be in a specific $D_k$ iff $||x_i - \\mu_k||_2^2 \\leq ||x_i - \\mu_j||_2^2$ for $k\\neq j$ meaning that a certain point is allocated to a specific centroid onlt if it is the closest one to it. So:\n",
    "\n",
    "$$ \\argmin_{\\mu_k}\\sum_{k=1}^K\\sum_{i=1}^N[x_i\\in D_k]\\cdot||x_i - \\mu_k||_2^2 = \\argmin_{\\mu_k}\\sum_{k=1}^K\\left[||x_i - \\mu_k||_2^2 \\leq ||x_i - \\mu_j||_2^2\\text{ for }k\\neq j \\right]\\cdot||x_i - \\mu_k||_2^2$$\n",
    "\n",
    "Following the same logic we can also say that:\n",
    "\n",
    "$$\\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{i = 1}^{N} \\min_{k} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} =\\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{i = 1}^{N}\\sum_{k=1}^{K}\\left[k=arg\\min_{j}‚Äñùíô_{i}-\\mu_{j}‚Äñ^{2}\\right]\\left|\\left|x_{i}-\\mu_{k}\\right|\\right|_2^{2}$$\n",
    "\n",
    "Note that the two indicators are equivalent and therefore the two terms are equivalent.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Question\n",
    "\n",
    "Prove or disprove: The K-Means algorithm **always** converge to a global minimum."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Solution\n",
    "\n",
    "False.\n",
    "\n",
    "Consider the following counter example, with the $1D$ dataset:\n",
    "\n",
    "$$ x_1=0, x_2=3, x_3=5, x_4=6 $$\n",
    "\n",
    "Now let's run k-Means with $K=2$, with different centroids initialization:\n",
    "1. Initial centroids are: $\\mu_1=0, \\mu_2=6$.\n",
    "\n",
    "    The algorithm will converge after the first run to: $D_1=\\{x_1,x_2\\}, D_2=\\{x_3,x_4\\}$.\n",
    "\n",
    "    The new centroids will be: $\\mu_1=\\frac{0+3}{2}=1.5, \\mu_2=\\frac{5+6}{2}=5.5$\n",
    "\n",
    "    The objective value will be $(0-1.5)^2+(3-1.5)^2+(5-5.5)^2+(6-5.5)^2=5$\n",
    "\n",
    "    At the next step the points are assigned to the same centroids and the algorithm stops.\n",
    "\n",
    "2. If we initialize the centroids with $\\mu_1=0, \\mu_2=5$ the assignment after the first run will be: $D_1=\\{x_1\\}, D_2=\\{x_2,x_3,x_4\\},\\text{ new centroids: }\\mu_1=0, \\mu_2=\\frac{3+5+6}{3}=4.66$\n",
    "\n",
    "    At the next run the points will be assigned to the same centroids and the algorithm stops.\n",
    "\n",
    "    This time the objective function value will be: $(0-0)^2+(3-4.66)^2+(5-4.66)^2+(6-4.66)^2=4.66$.\n",
    "\n",
    "So, although both our runs converged, they yealded different objecftive cost, hence the algorithm dosen't always converge to a global minimum, rather it is dependent of the initial centroids.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Medians is a variation of the K-Means where the $ {L}_{1} $ norm is used instead of the squared $ {L}_{2} $:\n",
    "\n",
    "$$ \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{1} $$\n",
    "\n",
    "### 3.3. Question\n",
    "\n",
    "1. Derive _Step I_ of the K-Medians.  \n",
    "   Give an example where the assignment will be different than the assignment in K-Means.\n",
    "2. Derive _Step II_ of the K-Medians.  \n",
    "   Is the derived solution unique? Explain.  \n",
    "   Give a motivation for using the $ {L}_{1} $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Solution\n",
    "\n",
    "0. Choose k centroids.\n",
    "1. Assume fixed centroids, assign each data point to it nearest centroid, based on $L_1$ norm.\n",
    "\n",
    "    $\\{D_k\\}=\\operatorname{arg min}_{\\{D_k\\}}\\sum_{k=1}^K\\sum_{x_i\\in D_k}||x_i-\\mu_k||_1$\n",
    "    \n",
    "    $s=\\operatorname{arg min}_k ||x_i-\\mu_k||_1 \\implies x_i\\in D_s$\n",
    "2. Assume fixed clusters, calculate the new centroids which are the medians of each cluster.\n",
    "\n",
    "    $\\{\\mu_k\\}=\\operatorname{arg min}_{\\{\\mu_k\\}}\\sum_{k=1}^K\\sum_{x_i\\in D_k}||x_i-\\mu_k||_1$\n",
    "\n",
    "    $\\mu_k^*= \\operatorname{arg min}_{\\mu_k}\\sum_{x_i\\in D_k}||x_i-\\mu_k||_1$,  $k\\in\\{1,2...,K\\}$\n",
    "\n",
    "    Sort $\\{\\mu_k^*\\}$ and choose the median.\n",
    "\n",
    "    - If the number of elements in the cluster is odd, $\\mu_k^* = x_{\\frac{n+1}{2}}$\n",
    "    - If the number of elements in the cluster is even, $\\mu_k^* = \\frac{x_{\\frac{n}{2}}+x_{\\frac{n}{2}+1}}{2}$\n",
    "    \n",
    "3. Repeat steps 1 and 2 until convergence.\n",
    "\n",
    "We will want to use $L_1$ when we want the algorithm to be less effected by outliers.\n",
    "\n",
    "Consider the $1D$ dataset $\\{0,3,5,100\\}$ and the initial centroids $\\{0,5\\}$.\n",
    "\n",
    "- Using k-Means we will get in the first run the assignments of $C_1=\\{0\\}, C_2=\\{3,5,100\\}, \\mu=\\{0,36\\}$.\n",
    "    - At the next step we will have $C_1=\\{0,3,5\\}, C_2=\\{100\\}, \\mu=\\{2.66,100\\}$ and this will be the final assignment.\n",
    "- Using k-meadians we will get at the first run: $C_1=\\{0\\}, C_2=\\{3,5,100\\}, \\mu=\\{0,5\\} \\text{(since 5 is the median of cluster 2)}$.\n",
    "    - At the next run the centroids and the clusters will not change and this will be the final assignment.\n",
    "\n",
    "We can see that the two algorithms yealds diffierent results.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gaussian Mixture Model\n",
    "\n",
    " * Let $\\underline{X} \\sim \\mathcal{N}_{d} \\left( \\boldsymbol{\\mu}_{x}, {\\Sigma}_{x} \\right)$ be a Gaussian Radom Vector.\n",
    " * Let $Y = {\\boldsymbol{a}}^{T} \\underline{X} + b$ be a random variable.\n",
    "\n",
    "\n",
    "### 4.1. Question\n",
    "\n",
    "Find ${f}_{Y} \\left( y \\right)$, the Probability Density Function (PDF) of $Y$ as a function of $\\boldsymbol{\\mu}_{x}, {\\Sigma}_{x}, \\boldsymbol{a}, b$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Solution\n",
    "\n",
    "By the properties of the multivariate gaussian $X$, we know that $Y$ is a univariate gaussian (since $a^TX$ returns scalar that depends on $X$ and $b$ is a constant that only shifts the values), with PDF of the form:\n",
    "\n",
    "$$ f_Y(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}\\operatorname{exp}\\left(-\\frac{(y-\\mu_y)^2}{2\\sigma^2_y}\\right) $$\n",
    "\n",
    "We want to find the parameters of the function: $\\mu_y, \\sigma^2_y$\n",
    "\n",
    "$$ \\mu_y = \\mathbb{E}[Y] = \\mathbb{E}\\left[a^TX+b\\right] = a^T\\mathbb{E}[X] + b = a^T\\mu_x + b $$\n",
    "\n",
    "The second equality is by the liniarity of expectations and that $a^T$ is a constant.\n",
    "\n",
    "$$ \\sigma^2_y = Var[Y] = \\mathbb{E}\\left[(Y-\\mathbb{E}[Y])(Y-\\mathbb{E}[Y])^T\\right]$$\n",
    "$$ = \\mathbb{E}\\left[(a^TX+b-a^T\\mu_x-b)(a^TX+b-a^T\\mu_x-b)^T\\right] $$\n",
    "$$ = \\mathbb{E}\\left[a^T(X-\\mu_x)(X-\\mu_x)^Ta\\right] $$\n",
    "$$ = a^T\\mathbb{E}\\left[(X-\\mu_x)(X-\\mu_x)^T\\right] a $$\n",
    "$$ \\sigma^2_y = a^T\\Sigma_x a $$\n",
    "\n",
    "Plug back $\\mu_y, \\sigma^2_y$ into $f_Y(y)$ to get the PDF of $Y$ as function of $\\mu_x,\\Sigma_x,a,b$:\n",
    "\n",
    "$$ f_Y(y) = \\frac{1}{\\sqrt{2\\pi a^T\\Sigma_x a}}\\operatorname{exp}\\left(-\\frac{(y-a^T\\mu_x - b)^2}{2a^T\\Sigma_x a}\\right) $$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix $A \\in \\mathbb{R}^{d \\times d}$ is called Symmetric Positive Semi Definite (SPSD) if ${A}^{T} = A$ and for any $\\boldsymbol{v} \\in \\mathbb{R}^{d}$:\n",
    "\n",
    "$$ \\boldsymbol{v}^{T} A \\boldsymbol{v} \\geq 0 $$\n",
    "\n",
    "### 4.2. Question\n",
    "\n",
    "Let $\\underline{X}$ be a random vector with covariance matrix ${\\Sigma}_{x}$. Show that ${\\Sigma}_{x}$ is an SPSD matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove that $\\Sigma_x$ is SPSD we will show that it satisfies both properties of SPSD:\n",
    "1. Symmetry:\n",
    "    \n",
    "    By definition:\n",
    "    $$\\Sigma_x=\\mathbb{E}\\left[(X-\\mu_x)(X-\\mu_x)^T\\right]$$\n",
    "    $$\\Sigma_x^T = \\left(\\mathbb{E}\\left[(X-\\mu_x)(X-\\mu_x)^T\\right]\\right)^T$$\n",
    "    $$ = \\mathbb{E}\\left[\\left((X-\\mu_x)(X-\\mu_x)^T\\right)^T\\right]$$\n",
    "    $$ = \\mathbb{E}\\left[(X-\\mu_x)(X-\\mu_x)^T\\right] \\text{ (by rules of transpose)}$$\n",
    "    $$ \\implies \\Sigma_x^T = \\Sigma_x $$\n",
    "\n",
    "2. $\\forall v\\in \\mathbb{R}^d, v^T\\Sigma_x v \\geq 0$ :\n",
    "    $$ v^T\\Sigma_x v = v^T\\mathbb{E}\\left[(X-\\mu_x)(X-\\mu_x)^T\\right]v $$\n",
    "    $$ = \\mathbb{E}\\left[v^T(X-\\mu_x)(X-\\mu_x)^Tv\\right] $$\n",
    "    $$ = \\mathbb{E}\\left[(v^T(X-\\mu_x))^2\\right] \\geq 0$$\n",
    "    $$ (v^T(X-\\mu_x))^2 \\geq 0 \\forall v $$\n",
    "    $$ \\implies v^T\\Sigma_x v \\geq 0 $$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic form $ \\left \\langle \\boldsymbol{x}, \\boldsymbol{A} \\boldsymbol{x} \\right \\rangle = \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} $ is a common operation in algebra and appears in the PDF of the Gaussian distribution.\n",
    "\n",
    "### 4.3. Question \n",
    "\n",
    "For $ \\boldsymbol{x} \\in \\mathbb{R}^{2} $ define $ Q \\left( \\boldsymbol{x} \\right) = 3 {x}_{1}^{2} + 2 {x}_{1} {x}_{2} - 5 {x}_{2}^{2} $.\n",
    "\n",
    "1. Find a symmetric matrix $ \\boldsymbol{A} $ which defines the quadratic form.\n",
    "2. Prove / Disprove: The matrix is unique.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You may find the decomposition of each matrix into symmetric and anti symmetric useful: $ \\boldsymbol{B} = \\boldsymbol{B}_{s} + \\boldsymbol{B}_{a} $ where $ \\boldsymbol{B}_{s} = \\frac{1}{2} \\left( \\boldsymbol{B} + \\boldsymbol{B}^{T} \\right) $ and $ \\boldsymbol{B}_{a} = \\frac{1}{2} \\left( \\boldsymbol{B} - \\boldsymbol{B}^{T} \\right) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Solution\n",
    "\n",
    "1. We want to find matrix $A$ such that $x^TAx = Q(x)$.\n",
    "    If we write it explicitly:\n",
    "    $$Q(x) = \\begin{pmatrix}x_1 & x_2\\end{pmatrix}\\begin{pmatrix}a & b\\\\ c & d\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}$$\n",
    "    $$ = \\begin{pmatrix}ax_1+cx_2 & bx_1+dx_2\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix} $$\n",
    "    $$ = ax_1^2+cx_1x_2+bx_1x_2+dx_2^2 $$\n",
    "    $$= ax_1^2 +(b+c)x_1x_2 +dx_2^2 = 3x_1^2 +2x_1x_2 -5 x_2^2$$\n",
    "    $$\\implies a=3, d=-5, b+c=2 \\text{ but we want }A\\text{ to be symmetric so }b=c=1$$\n",
    "    $$\\implies A=\\begin{pmatrix}3&1\\\\1&-5\\end{pmatrix}$$\n",
    "\n",
    "2. To prove that $A$ is unique let's define matrix $B$ such that $x^TAx=x^TBx \\text{ for all } x\\in\\mathbb{R}^2$\n",
    "    $$ x^TAx - x^TBx = 0 $$\n",
    "    $$ x^T(A-B)x = 0 $$\n",
    "    This must hold for all $x\\in\\mathbb{R}^2$ so let's take $e_1=\\begin{pmatrix}1\\\\0\\end{pmatrix}$ and $e_2=\\begin{pmatrix}0\\\\1\\end{pmatrix}$. We get:\n",
    "    $$\\begin{pmatrix}1&0\\end{pmatrix}(A-B)\\begin{pmatrix}1\\\\0\\end{pmatrix}=0$$\n",
    "    $$\\text{and}$$\n",
    "    $$\\begin{pmatrix}0&1\\end{pmatrix}(A-B)\\begin{pmatrix}0\\\\1\\end{pmatrix}=0$$\n",
    "\n",
    "    The only solution to this system of equations is the zero matrix, hence $A=B$ so $A$ is unique.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hierarchical Clustering\n",
    "\n",
    "The _complete linkage distance_ between the 2 clusters $\\mathcal{C}_{1} = {\\left\\{ \\boldsymbol{x}_{i} \\right\\}}_{i = 1}^{{N}_{1}}$ and $\\mathcal{C}_{2} = {\\left\\{ \\boldsymbol{x}_{j} \\right\\}}_{j = 1}^{{N}_{2}}$:\n",
    "\n",
    "$$ {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) = \\begin{cases}\n",
    "0 & \\text{ if } \\mathcal{C}_{1} = \\mathcal{C}_{2} \\\\ \n",
    "\\max_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| & \\text{ if } \\mathcal{C}_{1} \\neq \\mathcal{C}_{2}\n",
    "\\end{cases} $$\n",
    "\n",
    "### 5.1. Question\n",
    "\n",
    "Prove that the complete linkage is indeed a metric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Solution\n",
    "\n",
    "We need to show that complete linkage distance satisfy the 4 properties of a metric:\n",
    "1. Identity, $d(x,y)=0 \\implies x=y$:\n",
    "    \n",
    "    By the definition of complete linkage, if $C_1=C_2$ than $d^2=0$.\n",
    "    \n",
    "    If $C_1\\neq C_2$ than exists $x_i\\in C_1, x_i\\notin C_2$, and exists $x_j\\in C_2, x_j\\notin C_1$\n",
    "\n",
    "    By definition of $L_2$ norm, $||x_i-x_j||\\geq 0$ and in practicular $||x_i-x_j||= 0$ only when $x_i=x_j$.\n",
    "\n",
    "    Therefore, $d^2_{Complete Link}(C_1,C_2)=0$ iff $C_1=C_2$.\n",
    "\n",
    "2. Non-negetivity, $d(x,y)\\geq 0$:\n",
    "    As we showed above d^2_{Complete Link}(C_1,C_2)\\geq 0$\n",
    "\n",
    "3. Symmetry, $d(x,y)=d(y,x)$:\n",
    "    Norm is symmetric: $||(x_i-x_j)||=||-(x_i-x_j)||$.\n",
    "\n",
    "    Hence $d^2_{Complete Link}(C_1,C_2) = d^2_{Complete Link}(C_2,C_1)$\n",
    "\n",
    "4. Triangle Inequality, $d(x,y)+d(y,z)\\leq d(x,z)$:\n",
    "\n",
    "    Let $C_1,C_2,C_3$ be 3 unique clusters (if any two are identical then the solution is trivial). and Let $x_1\\in C_1, x_2\\in C_2, x_3\\in C_3$.\n",
    "\n",
    "    By definition of norm, $||x_1-x_3|| \\leq ||x_1-x_2|| + ||x_2-x_3||$.\n",
    "\n",
    "    Now let's set $x_1, x_3$ to be the two farthest points between $C_1$ and $C_3$, such that $d^2_{Complete Link}(C_1,C_3) = ||x_1-x_3||$.\n",
    "\n",
    "    By definition of complete linkage, w.l.o.g. we have for any $x_2$:\n",
    "    $$||x_1-x_2||\\leq max_{x_i\\in C_1,x_j\\in C_2}||x_i - x_j|| = d^2_{Complete Link}(C_1,C_2)$$\n",
    "    $$||x_2-x_3||\\leq max_{x_j\\in C_2,x_k\\in C_3}||x_j - x_k|| = d^2_{Complete Link}(C_2,C_3)$$\n",
    "\n",
    "    Then we have:\n",
    "\n",
    "    $$ d^2_{Complete Link}(C_1,C_3) = ||x_1-x_3|| \\leq ||x_1-x_2|| + ||x_2-x_3|| \\leq d^2_{Complete Link}(C_1,C_2) + d^2_{Complete Link}(C_2,C_3) $$\n",
    "\n",
    "We showed that complete linkage satisfies all the conditions of a distance metric, hence it is indeed a valid metric.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * The _single linkage dissimilarity_ between the 2 clusters $\\mathcal{C}_{1} = {\\left\\{ \\boldsymbol{x}_{i} \\right\\}}_{i = 1}^{{N}_{1}}$ and $\\mathcal{C}_{2} = {\\left\\{ \\boldsymbol{x}_{j} \\right\\}}_{j = 1}^{{N}_{2}}$:\n",
    "\n",
    "$$ {d}^{2}_{\\text{Single Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) = \\min_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| $$\n",
    "\n",
    " * The _Lance Williams_ update rule is given by: ${D}_{\\overline{ij}, k} = {\\alpha}_{i} {D}_{i, k} + {\\alpha}_{j} {D}_{j, k} + \\beta {D}_{i, j} + \\gamma \\left| {D}_{i, k} - {D}_{j, k} \\right|$.\n",
    "\n",
    "### 5.2. Question\n",
    "\n",
    "Consider 3 clusters $\\mathcal{C}_{1}, \\mathcal{C}_{2}, \\mathcal{C}_{3}$ with ${D}_{i, j} = {d}_{\\text{Single Link}} \\left( \\mathcal{C}_{i}, \\mathcal{C}_{j} \\right)$.  \n",
    "Prove that:\n",
    "\n",
    "$$ {D}_{\\overline{12}, 3} = {d}_{\\text{Single Link}} \\left( \\mathcal{C}_{1} \\cup \\mathcal{C}_{2}, \\mathcal{C}_{3} \\right) $$\n",
    "\n",
    "In other words, show that the _Lance Williams_ algorithm is correct for the _single linkage dissimilarity_."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Solution\n",
    "\n",
    "Let $\\alpha_i=0.5, \\alpha_j=0.5, \\beta=0, \\gamma=-0.5$.\n",
    "\n",
    "Then we got:\n",
    "$$ D_{\\overline{12},3}=0.5D_{1,3}+0.5D_{2,3}-0.5|D_{1,3}-D_{2,3}| $$\n",
    "\n",
    "Since $d_{Single Link}(C_1\\cup C_2, C_3) = min_{x_i\\in C_1\\cup C_2,x_j\\in C_3}||x_i-x_j||$ we have 3 cases to consider:\n",
    "\n",
    "1. $x_i\\in C_1$:\n",
    "    \n",
    "    $ d_{Single Link}(C_1\\cup C_2, C_3) = d_{Single Link}(C_1, C_3) = D_{1,3}$\n",
    "    \n",
    "    and\n",
    "    \n",
    "    $ D_{1,3}\\leq D_{2,3} \\implies |D_{1,3} - D_{2,3}| = D_{2,3} - D_{1,3}$\n",
    "    \n",
    "    so:\n",
    "    \n",
    "    $ D_{\\overline{12},3}=0.5D_{1,3}+0.5D_{2,3}-0.5(D_{2,3}-D_{1,3}) =  D_{1,3}$\n",
    "\n",
    "2. $x_i\\in C_2$:\n",
    "    \n",
    "    $ d_{Single Link}(C_1\\cup C_2, C_3) = d_{Single Link}(C_2, C_3) = D_{2,3}$\n",
    "    \n",
    "    and\n",
    "    \n",
    "    $ D_{2,3}\\leq D_{1,3} \\implies |D_{1,3} - D_{2,3}| = D_{1,3} - D_{2,3}$\n",
    "    \n",
    "    so:\n",
    "    \n",
    "    $ D_{\\overline{12},3}=0.5D_{1,3}+0.5D_{2,3}-0.5(D_{1,3} - D_{2,3}) =  D_{2,3}$\n",
    "\n",
    "3. A tie between $C_2$ and $C_3$:\n",
    "\n",
    "    $ d_{Single Link}(C_1\\cup C_2, C_3) = d_{Single Link}(C_2, C_3) = D_{2,3} = d_{Single Link}(C_1, C_3) = D_{1,3} $\n",
    "    \n",
    "    so:\n",
    "    \n",
    "    $ D_{\\overline{12},3}=0.5D_{1,3}+0.5D_{2,3} = 0.5D_{1,3}+0.5D_{1,3} = D_{1,3}$\n",
    "\n",
    "After considering and proving all the cases, we can say that: $D_{\\overline{12},3} = d_{Single Link}(C_1\\cup C_2, C_3)$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

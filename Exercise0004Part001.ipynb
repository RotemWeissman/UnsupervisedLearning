{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 004 - Part I\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 08/09/2023 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_08/Exercise0004Part001.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
    " - Answer all questions / tasks within the Jupyter Notebook.\n",
    " - Use MarkDown + MathJaX + Code to answer.\n",
    " - Verify the rendering on VS Code.\n",
    " - Submission in groups (Single submission per group).\n",
    " - The submission files should have the format: `<fileName>_GRP_<#>`.  \n",
    "   For instance, `Exercise001Part002_GRP_A.ipynb` or `AuxFun_GRP_A.py`.\n",
    " - You may and _should_ use the forums for questions.\n",
    " - Good Luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "\n",
    " - `<FULL>_<NAME>_<ID001>`.\n",
    " - `<FULL>_<NAME>_<ID002>`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classic Multi Dimensional Scaling (MDS)\n",
    "\n",
    " * Given a function $ \\phi \\left( \\cdot \\right) : \\mathbb{R}^{D} \\to \\mathbb{R}^{M} $.\n",
    " * Consider the following inner product: ${\\left \\langle \\boldsymbol{x}, \\boldsymbol{y} \\right \\rangle}_{\\phi} = \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle$.\n",
    " * Yields the induced norm: $ {\\left\\| \\boldsymbol{x} \\right\\|}_{\\phi} = \\sqrt{ \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{x} \\right) \\right \\rangle } $.\n",
    " * Yields the induced metric: ${d}_{\\phi} \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left\\| \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{\\phi}$.\n",
    "\n",
    "### 1.1. Question\n",
    "\n",
    "Consider the data (Training set) $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{R}^{D} \\right\\}_{i = 1}^{N}$ and let $\\boldsymbol{D}_{\\phi} \\left[ i, j \\right] = {d}_{\\phi}^{2} \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$.\n",
    "\n",
    "Show that $- \\frac{1}{2} \\boldsymbol{J} \\boldsymbol{D}_{\\phi} \\boldsymbol{J} = J \\boldsymbol{K}_{\\phi} \\boldsymbol{J}$ where:\n",
    "\n",
    " * $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}$ - The centering matrix.\n",
    " * $\\boldsymbol{K}_{\\phi} = \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}$ where: $\\boldsymbol{\\Phi} = \\begin{bmatrix} \\mid & \\mid &  & \\mid \\\\ \\phi \\left( \\boldsymbol{x}_{1} \\right) & \\phi \\left( \\boldsymbol{x}_{2} \\right) & \\dots & \\phi \\left( \\boldsymbol{x}_{N} \\right) \\\\ \\mid & \\mid & & \\mid \\end{bmatrix} \\in \\mathbb{R}^{M \\times N} = \\boldsymbol{\\Phi} = \\begin{bmatrix} \\mid & \\mid &  & \\mid \\\\ \\boldsymbol{\\phi}_{1} & \\boldsymbol{\\phi}_{2} & \\dots & \\boldsymbol{\\phi}_{N} \\\\ \\mid & \\mid & & \\mid \\end{bmatrix} \\in \\mathbb{R}^{M \\times N}$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Hints:\n",
    "    * Show that the transformation $\\phi \\left( \\cdot \\right)$ must be linear: $\\phi \\left( \\alpha \\boldsymbol{x}, \\beta \\boldsymbol{y} \\right) = \\alpha \\phi \\left( \\boldsymbol{x} \\right) + \\beta \\phi \\left( \\boldsymbol{y} \\right)$.  \n",
    "      You may use $\\left \\langle \\alpha \\boldsymbol{x} + \\beta \\boldsymbol{y}, \\boldsymbol{z} \\right \\rangle$ as a starting point. \n",
    "    * Show that ${d}_{\\phi}^{2} \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left\\| \\phi \\left( \\boldsymbol{x} \\right) - \\phi \\left( \\boldsymbol{y} \\right) \\right\\|}_{2}^{2} = {\\left\\| \\phi \\left( \\boldsymbol{x} \\right) \\right\\|}_{2}^{2} - 2 \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle + {\\left\\| \\phi \\left( \\boldsymbol{y} \\right) \\right\\|}_{2}^{2}$\n",
    "    * Use the lecture notes to conclude $- \\frac{1}{2} \\boldsymbol{J} \\boldsymbol{D}_{\\phi} \\boldsymbol{J} = J \\boldsymbol{K}_{\\phi} \\boldsymbol{J}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the data (Training set) $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{R}^{D} \\right\\}_{i = 1}^{N}$ and let $\\boldsymbol{D} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2}$.\n",
    "\n",
    "### 1.2. Question\n",
    "\n",
    "Show that $\\boldsymbol{v}^{T} \\boldsymbol{D} \\boldsymbol{v} \\leq 0$ for any $\\boldsymbol{v}$ such that $\\left \\langle \\boldsymbol{v}, \\boldsymbol{1} \\right \\rangle = 0$.  \n",
    "What does it imply on _distance matrices_?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Majorization Minimization / Maximization\n",
    "\n",
    "Consider the matrix:\n",
    "\n",
    "$$ \\boldsymbol{Y} \\left[ i, j \\right] = \\begin{cases} \\boldsymbol{X} \\left[ i, j \\right] & \\text{ if } \\boldsymbol{M} \\left[ i, j \\right] = 1 \\\\ 0 & \\text{ if } \\boldsymbol{M} \\left[ i, j \\right] = 0 \\end{cases} $$\n",
    "\n",
    "Namely $\\boldsymbol{Y} = \\boldsymbol{M} \\circ \\boldsymbol{X}$ where $\\circ$ is the Hadamard Product and $\\boldsymbol{M} \\in {\\left\\{ 0, 1 \\right\\}}^{M \\times N}$ is a _binary mask matrix_.\n",
    "\n",
    "Given $\\boldsymbol{Y} \\in \\mathbb{R}^{M \\times N}$, the low rank matrix completion objective is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg \\min_{\\boldsymbol{X} \\in \\mathbb{R}^{M \\times N}} \\quad & {\\left\\| \\boldsymbol{M} \\circ \\left( \\boldsymbol{X} - \\boldsymbol{Y} \\right) \\right\\|}_{F}^{2} \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\operatorname{Rank} \\left( \\boldsymbol{X} \\right) & \\leq d \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### 2.1. Question (Bonus 2 Points)\n",
    "\n",
    "Consider the following function:\n",
    "\n",
    "$$ g \\left( \\boldsymbol{X}, \\boldsymbol{Z} \\right) = {\\left\\| \\boldsymbol{X} - \\boldsymbol{Z} + \\boldsymbol{M} \\circ \\left( \\boldsymbol{Z} - \\boldsymbol{Y} \\right) \\right\\|}_{F}^{2} $$\n",
    "\n",
    "Show that $g \\left( \\cdot, \\cdot \\right)$ surrogates the objective function $f \\left( \\boldsymbol{X} \\right) = {\\left\\| \\boldsymbol{M} \\circ \\left( \\boldsymbol{X} - \\boldsymbol{Y} \\right) \\right\\|}_{F}^{2}$.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You may show $g \\left( \\boldsymbol{X}, \\boldsymbol{Z} \\right) = {\\left\\| \\boldsymbol{M} \\circ \\left( \\boldsymbol{X} - \\boldsymbol{Y} \\right) + \\tilde{\\boldsymbol{M}} \\circ \\left( \\boldsymbol{X} - \\boldsymbol{Z} \\right) \\right\\|}_{F}^{2}$ where $\\tilde{\\boldsymbol{M}} = \\boldsymbol{1} \\boldsymbol{1}^{T} - \\boldsymbol{M}$ is the complement of $\\boldsymbol{M}$, as an intermediate step.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Metric Multi Dimensional Scaling (MDS)\n",
    "\n",
    "The metric MDS objective is given by:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N}} {\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} $$\n",
    "\n",
    "Where:\n",
    "\n",
    " * $\\boldsymbol{\\Delta}_{x} \\left[ i, j \\right] = d \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ - The given distance matrix.\n",
    " * $\\boldsymbol{D}_{z} = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}$.\n",
    "\n",
    "Consider the surrogate function:\n",
    "\n",
    "$$ g \\left( \\boldsymbol{Z}, \\tilde{\\boldsymbol{Z}} \\right) = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} + 2 N \\operatorname{Tr} \\left( \\boldsymbol{Z} \\boldsymbol{J} \\boldsymbol{Z}^{T} \\right) - 4 \\left \\langle \\boldsymbol{Z}^{T} \\tilde{\\boldsymbol{Z}}, \\boldsymbol{B} \\right \\rangle $$\n",
    "\n",
    "Where:\n",
    "\n",
    " * $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}$ - The centering matrix.\n",
    " * $\\tilde{\\boldsymbol{D}}_{\\tilde{z}} \\left[ i, j \\right] = {\\left\\| \\tilde{\\boldsymbol{z}}_{i} - \\tilde{\\boldsymbol{z}}_{j} \\right\\|}_{2}$.\n",
    " * $\\boldsymbol{C} \\left[ i, j \\right] = \\begin{cases} 0 & \\text{ if } i = j \\\\ - \\frac{ \\boldsymbol{\\Delta}_{x} \\left[ i, j \\right] }{ \\tilde{\\boldsymbol{D}}_{z} \\left[ i, j \\right] } & \\text{ if } i \\neq j \\end{cases}$.\n",
    " * $\\boldsymbol{B} = \\boldsymbol{C} - \\operatorname{Diag} \\left( \\boldsymbol{C} \\boldsymbol{1} \\right)$.\n",
    "\n",
    "### 3.1. Question\n",
    "\n",
    "Prove that $ \\boldsymbol{B} \\boldsymbol{J} = \\boldsymbol{B} $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Question\n",
    "\n",
    "Show that $g \\left( \\boldsymbol{Z}, \\boldsymbol{Z} \\right) = {\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2}$.\n",
    "\n",
    "\n",
    " * <font color='brown'>(**#**)</font> Hints (See _lecture notes_):\n",
    "     * ${\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} - 2 \\left \\langle \\boldsymbol{\\Delta}_{x}, \\boldsymbol{D}_{z} \\right \\rangle + {\\left\\| \\boldsymbol{D}_{z} \\right\\|}_{F}^{2}$.\n",
    "     * ${\\left\\| \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} = 2 N \\operatorname{Tr} \\left( \\boldsymbol{Z} \\boldsymbol{J} \\boldsymbol{Z}^{T} \\right)$.\n",
    "     * $\\boldsymbol{D}^{\\circ 2}_{z} \\left[ i, j \\right] = \\boldsymbol{p} \\boldsymbol{1}^{T} - 2 \\boldsymbol{Z}^{T} \\boldsymbol{Z} + 1 \\boldsymbol{p}^{T}, \\; \\boldsymbol{p} = \\begin{bmatrix} {\\left\\| \\boldsymbol{z}_{1} \\right\\|}_{2}^{2} \\\\ {\\left\\| \\boldsymbol{z}_{2} \\right\\|}_{2}^{2} \\\\ \\vdots {\\left\\| \\boldsymbol{z}_{N} \\right\\|}_{2}^{2} \\end{bmatrix}$.\n",
    "     * For $\\tilde{\\boldsymbol{Z}} = \\boldsymbol{Z}$ we have $\\left \\langle \\boldsymbol{\\Delta}_{x}, \\boldsymbol{D}_{z} \\right \\rangle = - \\left \\langle \\boldsymbol{C}, \\boldsymbol{D}^{\\circ 2}_{z} \\right \\rangle$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IsoMap\n",
    "\n",
    "Let $G = \\left( V, E, W \\right) $ be a simple, undirected and weighted graph with no negative weights / edges.  \n",
    "Let $\\boldsymbol{D} \\in \\mathbb{R}^{N \\times N}$ be the shortest path distance matrix where $ \\left| V \\right| = N$.\n",
    "\n",
    "\n",
    "### 4.1. Question\n",
    "\n",
    "Prove or disprove: There is an embedding ${\\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{d} \\right\\}}_{i = 1}^{N}$ for some $d \\in \\mathbb{N}$ such that:\n",
    "\n",
    "$$ \\forall i, j \\; \\boldsymbol{D} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Question\n",
    "\n",
    " * Let $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$ be the training set.\n",
    " * Let $\\mathcal{Z} = \\left\\{ \\boldsymbol{z}_{i} \\right\\}_{i = 1}^{N}$ be the representation obtained by IsoMap (Encoded data).\n",
    " * Consider a new point $\\boldsymbol{x}^{\\ast}$ where $\\boldsymbol{x}^{\\ast} = \\boldsymbol{x}_{k}, \\; k \\in \\left\\{ 1, 2, \\ldots, N \\right\\}$.\n",
    " * Consider a new point $\\boldsymbol{x}^{\\dagger}$ where $\\nexists l : \\, \\boldsymbol{x}^{\\dagger} = \\boldsymbol{x}_{l}, \\; l \\in \\left\\{ 1, 2, \\ldots, N \\right\\}$.\n",
    " * Let $\\boldsymbol{z}^{\\ast}$ be the out of sample encoding applied to $\\boldsymbol{x}^{\\ast}$.\n",
    " * Let $\\boldsymbol{z}^{\\dagger}$ be the out of sample encoding applied to $\\boldsymbol{x}^{\\dagger}$.\n",
    "\n",
    "1. Prove or disprove: $\\boldsymbol{z}^{\\ast} = \\boldsymbol{z}_{k}$.\n",
    "1. Prove or disprove: Must exists $l$ such that $\\boldsymbol{z}^{\\dagger} = \\boldsymbol{z}_{l}$.\n",
    "\n",
    " * <font color='brown'>(**#**)</font> The out of sample encoding is as shown in _lecture notes_.\n",
    " * <font color='brown'>(**#**)</font> The question is basically if the out of sample extension is equivalent to having the point in the training set for such case.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Laplacian Eigenmaps\n",
    "\n",
    " * Let $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$.\n",
    " * Let $G = \\left( V, E, W \\right)$ be a weighted graph with with $V = \\mathcal{X}$.\n",
    " * Define $\\boldsymbol{W} \\left[ i, j \\right] = \\begin{cases} \\exp \\left( - \\frac{ {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} }{2 {\\sigma}^{2}} \\right) & \\text{ if } \\boldsymbol{x}_{i} \\in \\mathcal{N}_{j} \\text{ or } \\boldsymbol{x}_{j} \\in \\mathcal{N}_{i} \\\\ 0 & \\text{ else } \\end{cases}$.\n",
    " * Then ${e}_{ij} \\in E$ if $\\boldsymbol{W} \\left[ i, j \\right] \\neq 0$.\n",
    " * The _Graph Laplacian_ $\\boldsymbol{L} = \\boldsymbol{D} - \\boldsymbol{W}$.\n",
    " * The _Degree Matrix_ $\\boldsymbol{D} = \\operatorname{Diag} \\left( \\boldsymbol{W} \\boldsymbol{1} \\right)$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1. Question\n",
    "\n",
    " * Let $\\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N}$ be a set of data with $\\boldsymbol{z}_{i}$ being the $i$ -th column of $\\boldsymbol{Z}$.\n",
    " * Define $\\boldsymbol{D}_{z} \\in \\mathbb{R}^{N \\times N}$ where $\\boldsymbol{D}_{z} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}^{2}$.  \n",
    "\n",
    "Show that:\n",
    "\n",
    "$$ \\frac{1}{2} \\left \\langle \\boldsymbol{W}, \\boldsymbol{D}_{z} \\right \\rangle = \\operatorname{Tr} \\left(  \\boldsymbol{Z} \\boldsymbol{L} \\boldsymbol{Z}^{T} \\right) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Solution\n",
    "Let's start with the left-hand side of the equation, similar to what we saw in class:\n",
    "\n",
    "$$\\frac{1}{2}\\langle\\boldsymbol{W},\\boldsymbol{D}_z\\rangle = \\frac{1}{2}\\sum_{i,j}\\boldsymbol{W}[i,j]\\cdot||z_i-z_j||_2^2$$\n",
    "\n",
    "Now we'll expand the right-hand side of the equation:\n",
    "\n",
    "$$\\operatorname{Tr}\\left( \\boldsymbol{Z}\\boldsymbol{L}\\boldsymbol{Z}^T\\right)= \\operatorname{Tr}\\left(\\boldsymbol{Z}\\left(\\boldsymbol{D}-\\boldsymbol{W}\\right)\\boldsymbol{Z}^T\\right)$$\n",
    "$$= \\operatorname{Tr}\\left(\\boldsymbol{Z}\\boldsymbol{D}\\boldsymbol{Z}^T\\right) - \\operatorname{Tr}\\left(\\boldsymbol{Z}\\boldsymbol{W}\\boldsymbol{Z}^T\\right)$$\n",
    "\n",
    "Recall that for matrices $A$ and $B$ the trace is: $\\operatorname{Tr}(\\boldsymbol{A}\\boldsymbol{B})=\\sum_i a_i\\cdot b_i$\n",
    "\n",
    "Now let's consider the term $\\operatorname{Tr}\\left(\\boldsymbol{Z}\\boldsymbol{W}\\boldsymbol{Z}^T\\right)$ for a fixed $i$:\n",
    "\n",
    "$$z_i\\cdot\\left(\\boldsymbol{W}\\boldsymbol{Z}^T\\right)_i = \\sum_j\\boldsymbol{W}[i,j]z_i\\cdot z_j$$\n",
    "\n",
    "Now recall the definition of the squared Euclidean distance:\n",
    "\n",
    "$$||z_i-z_j||_2^2 = z_i\\cdot z_i + z_j\\cdot z_j - 2z_i\\cdot z_j$$\n",
    "$$ z_i\\cdot z_j = \\frac{1}{2}\\left(||z_i||^2+||z_j||^2-||z_i-z_j||^2_2\\right)$$\n",
    "\n",
    "So, \n",
    "$$z_i\\cdot\\left(\\boldsymbol{W}\\boldsymbol{Z}^T\\right)_i = \\frac{1}{2}\\sum_j\\boldsymbol{W}[i,j]\\left(||z_i||^2+||z_j||^2-||z_i-z_j||^2_2\\right)$$\n",
    "\n",
    "When summing over all $i$ we get the trace:\n",
    "$$ \\operatorname{Tr}\\left(\\boldsymbol{Z}\\boldsymbol{W}\\boldsymbol{Z}^T\\right) = \\frac{1}{2}\\sum_{i,j}\\boldsymbol{W}[i,j]\\left(||z_i||^2+||z_j||^2-||z_i-z_j||^2_2\\right)$$\n",
    "Looking on the $\\operatorname{Tr}\\left(\\boldsymbol{Z}\\boldsymbol{D}\\boldsymbol{Z}^T\\right)$:\n",
    "\n",
    "Since $\\boldsymbol{D}$ is a diagonal matrix, when multiply $\\boldsymbol{Z}$ by $\\boldsymbol{D}$, each column $z_i$ of $Z$ is scaled by the degree of node $i$, i.e. $D[i,i]$.\n",
    "\n",
    "$$\\operatorname{Tr}\\left(\\boldsymbol{Z}\\boldsymbol{D}\\boldsymbol{Z}^T\\right) = \\operatorname{Tr}\\left(\\sum_{i=1}^N \\boldsymbol{D}[i,i]z_iz_i^T\\right)=\\sum_{i=1}^N \\boldsymbol{D}[i,i]z_i\\cdot z_i = \\sum_{i=1}^N \\boldsymbol{D}[i,i]\\cdot||z_i||_2^2$$\n",
    "\n",
    "Given that $\\boldsymbol{D}[i,i] = \\sum_j\\boldsymbol{W}[i,j]$ and using the above expressions we can combine and get:\n",
    "\n",
    "$$ \\operatorname{Tr}\\left(\\boldsymbol{Z}\\boldsymbol{D}\\boldsymbol{Z}^T\\right) - \\operatorname{Tr}\\left(\\boldsymbol{Z}\\boldsymbol{W}\\boldsymbol{Z}^T\\right)$$\n",
    "\n",
    "$$ = \\sum_{i=1}^N \\boldsymbol{D}[i,i]\\cdot||z_i||_2^2 - \\frac{1}{2}\\sum_{i,j}\\boldsymbol{W}[i,j]\\left(||z_i||^2+||z_j||^2-||z_i-z_j||^2_2\\right)$$\n",
    "$$ = \\sum_{i,j}\\boldsymbol{W}[i,j]\\cdot||z_i||_2^2 - \\frac{1}{2}\\sum_{i,j}\\boldsymbol{W}[i,j]\\left(||z_i||^2+||z_j||^2-||z_i-z_j||^2_2\\right)$$\n",
    "$$ = \\frac{1}{2}\\sum_{i,j}\\boldsymbol{W}[i,j]\\cdot||z_i-z_j||_2^2$$\n",
    "$$ = \\frac{1}{2}\\langle\\boldsymbol{W},\\boldsymbol{D}_z\\rangle$$\n",
    "$$\\blacksquare$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $G$ be an undirected graph with a non negative weights with the corresponding graph laplacian matrix $\\boldsymbol{L}$.  \n",
    "The matrix $\\boldsymbol{L}$ has the eigen value $0$ with multiplicity of $k$.\n",
    "\n",
    "### 5.3. Question\n",
    "\n",
    "Show that the number of connected components of the graph is $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Solution\n",
    "\n",
    "As we saw in class, we know that $\\boldsymbol{L}\\succeq0$ with $0$ being its smallest eigenvalue.\n",
    "\n",
    "If we have $k$ connected components, we can write $L$ as a block diagonal matrix, such that all the vertices of a specific connected $C_i$ component are consecutive to one another. So the matrix $\\boldsymbol{L}$ will be:\n",
    "\n",
    "$$\\boldsymbol{L} = \\begin{bmatrix}\\boldsymbol{C}_1 &&& \\\\ & \\boldsymbol{C}_2 && \\\\ && \\ddots & \\\\ &&& \\boldsymbol{C}_k \\end{bmatrix}$$\n",
    "\n",
    "Each block $\\boldsymbol{C}_i$ corresponds to the connected component of vertices $\\{1,2,\\dots,a-1,a\\}$ where $a$ is the number of vertices in this components. \n",
    "\n",
    "Then, there is a corresponding vector $\\boldsymbol{v}_i$ of the form $\\begin{bmatrix} \\boldsymbol{0} \\\\ \\boldsymbol{1} \\\\ \\boldsymbol{1} \\end{bmatrix}$ with $0$ in the corresponding entries to the vertices in $L$:\n",
    "\n",
    "$$ \\begin{bmatrix}\\boldsymbol{C}_1 &&& \\\\ & \\boldsymbol{C}_2 && \\\\ && \\ddots & \\\\ &&& \\boldsymbol{C}_k \\end{bmatrix} \\begin{bmatrix} \\boldsymbol{0} \\\\ \\boldsymbol{1} \\\\ \\vdots \\\\ \\boldsymbol{1} \\end{bmatrix} = \\begin{bmatrix}\\boldsymbol{C}_1 &&& \\\\ & \\boldsymbol{C}_2 && \\\\ && \\ddots & \\\\ &&& \\boldsymbol{C}_k \\end{bmatrix} \\begin{bmatrix} \\boldsymbol{1} \\\\ \\boldsymbol{0} \\\\ \\vdots \\\\ \\boldsymbol{1} \\end{bmatrix} = \\begin{bmatrix}\\boldsymbol{C}_1 &&& \\\\ & \\boldsymbol{C}_2 && \\\\ && \\ddots & \\\\ &&& \\boldsymbol{C}_k \\end{bmatrix} \\begin{bmatrix} \\boldsymbol{1} \\\\ \\boldsymbol{1} \\\\ \\vdots \\\\ \\boldsymbol{0} \\end{bmatrix} = 0$$\n",
    "\n",
    "Since we have $k$ components, we have $k$ corresponding vectors that are orthogonal (linerarily indipendent) to each other, and $\\boldsymbol{Lv}_i = 0 = 0\\boldsymbol{v}_i$.\n",
    "\n",
    "Similarly, we can prove the other direction:\n",
    "\n",
    "If $L$ has the eigenvalue $0$ with multiplicity of $k$, that means that we have $k$ orthogonal vectors for which $\\boldsymbol{Lv}_i = 0 = 0\\boldsymbol{v}_i$.\n",
    "\n",
    "Since $L$ corresponds to vertices in the graph, we can arange the matrix such that each $\\boldsymbol{v}_i$ has entry $> 0$ in the corresponding rows to the indices of $\\boldsymbol{v}_i$ with $0$ in them, and $0$ otherwise.\n",
    "\n",
    "What we will get is a block diagonal matrix with $k$ components, that represent the connected components in the graph.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SNE & t-SNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ p $ be the perplexity hyper parameter of the _SNE_ algorithm.  \n",
    "\n",
    "### 6.2. Question\n",
    "\n",
    "* Given the perplexity, explain the motivation behind using $K$ or $\\epsilon$ graphs in the context of the algorithm?\n",
    "* How will the algorithm behave for $p \\to \\infty$?\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The definition for $K$ or $\\epsilon$ graphs is given in the lecture notes of _IsoMap_ and _Laplacian Eigenmaps_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Solution\n",
    "\n",
    "* We use perplexity to control the balance between local effect and global effect on the data embedding to the lower space.\n",
    "    * We want to use $K$ or $\\epsilon$ graphs to maintain the overall structure of the data, as we saw in other algorithms that deals with manifold learning.\n",
    "    * If we didn't use $K$ or $\\epsilon$ graphs, the similarity will only consider the euclidean distances and we could \"jump\" to points that are far along the manifold, with high probability.\n",
    "* When $p \\to \\infty$ the perplexity will favor more and more global structure, along the manifold, and the algorithm will be more similar to MDS.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
